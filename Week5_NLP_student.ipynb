{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 - Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this week's exercise, we will apply neural networks to a new type of data - text. You will also pick your own corpus of text to play with.  **But first, some definitions** (there's a lot of new jargon in 'NLP'):\n",
    "* *Corpus*: The set of all text documents you want to work on\n",
    "* *Document*: An individual unit of text in your corpus\n",
    "\n",
    "Below are some **examples of corpora** to get you thinking:\n",
    "\n",
    "* Corpus of 100,000 IMDB reviews, where each document is an individual review\n",
    "* Corpus of 20 English novels, where each document is an individual novel\n",
    "* Corpus of one Malay novel, where each document is a chapter\n",
    "\n",
    "You can **browse for a corpus in the below links** (if you don't already have one in mind):\n",
    "\n",
    "* https://github.com/niderhoff/nlp-datasets\n",
    "* https://www.gutenberg.org/catalog/\n",
    "\n",
    "If this is your first time working with text, it's probably easier to deal with a corpus of many short documents - for example the IMDB review dataset, which is linked below in Chapter 2.  Play around with several corpora over the course of week, and work with something that interests you. Remember text doesn't have to be English (try other languages), or even a natural language (try code or musical notation)!\n",
    "\n",
    "**Key learning resources** for the week:\n",
    "* https://web.stanford.edu/~jurafsky/slp3/ - legendary textbook introducing key theory and concepts of working with text, up to deep learning methods\n",
    "* http://web.stanford.edu/class/cs224n/ - great course that introduces theory and concepts of text processing in the context of deep learning (can read class notes / assignments and skip videos if you are short on time) \n",
    "* https://course.fast.ai/index.html - fast.ai's introduction to deep learning (you'll have to pick out the bits about text and RNNs) is an efficient and effective way of tackling the topic\n",
    "* https://www.datacamp.com/courses/natural-language-processing-fundamentals-in-python - very hands on datacamp course that will let you practice using existing tools for NLP tasks\n",
    "\n",
    "Some **additional tools below** that can help in NLP (if you haven't found them already):\n",
    "* scikit-learn has a handy set of features for NLP\n",
    "* https://spacy.io/ - commercially oriented python package for NLP\n",
    "* https://www.nltk.org/ - slightly more academic oriented python package for NLP \n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # a conventional alias\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "from src.preprocess import getFileNames\n",
    "from scipy.cluster import  hierarchy\n",
    "from scipy.cluster.hierarchy import linkage,dendrogram,cophenet\n",
    "from scipy.spatial.distance import pdist\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: How do we turn text into data we can use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert your corpus into bags of words\n",
    "\n",
    "We can't apply any of the techniques we have learned over the past few weeks directly on raw text.  Therefore, our first task is to convert our corpus into numbers.  The simplest way to do this is to use a **bag of words**. You can see some examples of this here: https://liferay.de.dariah.eu/tatom/index.html.\n",
    "\n",
    "Once you understand the concept, convert your corpus and documents into bags of words below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using the IMDB movies reviews\n",
    "- Unsup dataset is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mypath='/home/hanifa/workspace/AIAP/AIAP_Week5/aclImdb/train/tt'\n",
    "# mypath='/home/hanifa/workspace/AIAP/AIAP_Week5/aclImdb/unsup/'\n",
    "mypath='/home/jupyter/AIAP_Week5/aclImdb/unsup'\n",
    "filenames=getFileNames(mypath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkVectorizer(dtm):\n",
    "    a=dtm.sum(axis=0)\n",
    "    a.sort(axis=0)\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Was having memory errors loading all the unsup data, thus just took a sample of 7K.\n",
    "- Stopwords were iteratively added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text \n",
    "my_additional_stop_words=['br','film','movie']\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(my_additional_stop_words)\n",
    "\n",
    "vectorizer = CountVectorizer(input='filename',stop_words=stop_words,\n",
    "                             token_pattern='[a-zA-Z]{3,}',lowercase=True)\n",
    "\n",
    "    \n",
    "# vectorizer = CountVectorizer(input='filename',stop_words='english')'\n",
    "dtm = vectorizer.fit_transform(filenames)  # a sparse matrix \n",
    "vocab = vectorizer.get_feature_names()\n",
    "\n",
    "# type(dtm)                                         \n",
    "dtm = dtm.toarray()  # convert to a regular array \n",
    "vocab = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5556, 36082)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checkVectorizer(dtm)\n",
    "dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show us your bags\n",
    "\n",
    "Show and explain what one of your documents looks like as a bag of words below.  What are the advantages and disadvantages of encoding text as bags of words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The column represents the words and the rows the document itself. \n",
    "- By summing up the column, we can total frequency across all documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bag of Words Representation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaah</th>\n",
       "      <th>aahed</th>\n",
       "      <th>aames</th>\n",
       "      <th>aaron</th>\n",
       "      <th>aazmi</th>\n",
       "      <th>aback</th>\n",
       "      <th>abaddon</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandoning</th>\n",
       "      <th>abandonment</th>\n",
       "      <th>abandons</th>\n",
       "      <th>abaskharon</th>\n",
       "      <th>abattoir</th>\n",
       "      <th>abbas</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abbie</th>\n",
       "      <th>abbott</th>\n",
       "      <th>abbreviated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   aaa  aaah  aahed  aames  aaron  aazmi  aback  abaddon  abandon  abandoned  \\\n",
       "0    0     0      0      0      0      0      0        0        0          0   \n",
       "1    0     0      0      0      0      0      0        0        0          0   \n",
       "2    0     0      0      0      0      0      0        0        0          0   \n",
       "3    0     0      0      0      0      0      0        0        0          0   \n",
       "4    0     0      0      0      0      0      0        0        0          0   \n",
       "\n",
       "   abandoning  abandonment  abandons  abaskharon  abattoir  abbas  abbey  \\\n",
       "0           0            0         0           0         0      0      0   \n",
       "1           0            0         0           0         0      0      0   \n",
       "2           0            0         0           0         0      0      0   \n",
       "3           0            0         0           0         0      0      0   \n",
       "4           0            0         0           0         0      0      0   \n",
       "\n",
       "   abbie  abbott  abbreviated  \n",
       "0      0       0            0  \n",
       "1      0       0            0  \n",
       "2      0       0            0  \n",
       "3      0       0            0  \n",
       "4      0       0            0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame(dtm, columns=vocab)\n",
    "df[vocab[0:20]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clears Memory\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFrequentWord(dtm):\n",
    "    dtm_sum=dtm.sum(axis=0)\n",
    "    idx=dtm_sum.argmax(axis=0)\n",
    "    print(\"Most frequent word: %s\" %(vocab[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent word: like\n"
     ]
    }
   ],
   "source": [
    "getFrequentWord(dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cons:\n",
    " - Most frequent words are considered the most important\n",
    " - The context of the word is ignored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tell us a story with your bags\n",
    "Now that your text is in a more digestible format, you can apply previously learned techniques to better understand the corpus. **Create a brief story around your corpus, for example by using clustering techniques.** Some examples of what you can do below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use Hierarchical Clustering to understand similarity of documents in your corpus. What distance measure works best? Are the results what you expect?\n",
    "- Comparing Euclidean and Cosine, Cosine seems better as it <br/>\n",
    "[Hierarchial Clustering Code](https://datascience.stackexchange.com/questions/22828/clustering-with-cosine-similarity/22834)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Hierarchial Clustering drawbacks__\n",
    "- Hierarchial Clustering was not feasible on high dimensional datasets.\n",
    "- Below image was done on a small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doHierarchialClustering(dtm,metric,threshold):\n",
    "    Z = hierarchy.linkage(dtm,\"average\", metric=metric)\n",
    "    fig=plt.figure(figsize = (7,4))\n",
    "    ax=fig.add_subplot(111)\n",
    "    ax.set(title=\"method_str\",xlabel='Clusters',ylabel='Height')\n",
    "    dendrogram(Z,\n",
    "    #            labels=df1.index.values,\n",
    "           ax=ax,\n",
    "           truncate_mode='lastp',\n",
    "           orientation='top',\n",
    "           show_leaf_counts=True\n",
    "    )\n",
    "\n",
    "    plt.show()\n",
    "    C = hierarchy.fcluster(Z,threshold, criterion=\"distance\")\n",
    "    print(\"Number of clusters %d\" %(len(np.unique(hierarchy.fcluster(Z, threshold, criterion=\"distance\")))))\n",
    "    c, coph_dists = cophenet(Z, pdist(dtm))\n",
    "#     print (\"My name is %s and weight is %d kg!\" % ('Zara', 21))\n",
    "    print(\"Cophenet score is %f\"%(c))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters 2\n",
      "Cophenet score is 0.956333\n"
     ]
    }
   ],
   "source": [
    "doHierarchialClustering(dtm,'euclidean',threshold=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters 7\n",
      "Cophenet score is -0.235672\n"
     ]
    }
   ],
   "source": [
    "doHierarchialClustering(dtm,'cosine',threshold=.89)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Learn about *Latent Dirichlet Allocation* to extract topics from your corpora, and measure each document on how much of each topic it contains. How do you interpret these topics?\n",
    "\n",
    "Some **potential inspiration** below (but please keep your own story simple!):\n",
    "* https://liferay.de.dariah.eu/tatom/topic_model_mallet.html covers a few examples of text analysis\n",
    "* http://fantheory.viacom.com/\n",
    "* https://pudding.cool/2017/02/vocabulary/\n",
    "\n",
    "Additional resources on LDA (if you are interested): \n",
    "* https://medium.com/@lettier/how-does-lda-work-ill-explain-using-emoji-108abf40fa7d\n",
    "* https://www.youtube.com/watch?v=DDq3OVp9dNA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[LDA Code](https://medium.com/mlreview/topic-modeling-with-scikit-learn-e80d33668730)\n",
    "\n",
    "[Grid Search LDA](https://www.machinelearningplus.com/nlp/topic-modeling-python-sklearn-examples/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print (\"Topic %d:\" % (topic_idx))\n",
    "        print (\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
      "             evaluate_every=-1, learning_decay=0.7,\n",
      "             learning_method='online', learning_offset=10.0,\n",
      "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
      "             n_components=10, n_jobs=-1, n_topics=2, perp_tol=0.1,\n",
      "             random_state=100, topic_word_prior=None,\n",
      "             total_samples=1000000.0, verbose=0)\n",
      "Log Likelihood:  -4955014.817870218\n",
      "Perplexity:  6304.490987922088\n"
     ]
    }
   ],
   "source": [
    "n_components=2\n",
    "# lda_model= LatentDirichletAllocation(n_topics=n_components, max_iter=5, learning_method='online',\n",
    "#                                 learning_offset=50.,random_state=0,n_jobs=-1).fit(dtm)\n",
    "lda_model = LatentDirichletAllocation(n_topics=2,               # Number of topics\n",
    "                                      max_iter=10,               # Max learning iterations\n",
    "                                      learning_method='online',   \n",
    "                                      random_state=100,          # Random state\n",
    "                                      batch_size=128,            # n docs in each learning iter\n",
    "                                      evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n",
    "                                      n_jobs = -1,               # Use all available CPUs\n",
    "                                     )\n",
    "lda_output = lda_model.fit_transform(dtm)\n",
    "\n",
    "print(lda_model)  # Model attributes\n",
    "\n",
    "\n",
    "# # Log Likelyhood: Higher the better\n",
    "print(\"Log Likelihood: \", lda_model.score(dtm))\n",
    "\n",
    "# Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)\n",
    "print(\"Perplexity: \", lda_model.perplexity(dtm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='online', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
       "             n_components=10, n_jobs=-1, n_topics=2, perp_tol=0.1,\n",
       "             random_state=100, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__GridSearch the LDA Model to find best fit__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GridSearchLDA(dtm):\n",
    "    # Define Search Param\n",
    "    search_params = {'n_components': [10, 15, 20, 25, 30], 'learning_decay': [.5, .7, .9]}\n",
    "\n",
    "    # Init the Model\n",
    "    lda = LatentDirichletAllocation()\n",
    "\n",
    "    # Init Grid Search Class\n",
    "    model = GridSearchCV(lda, param_grid=search_params)\n",
    "\n",
    "    # Do the Grid Search\n",
    "    model.fit(dtm)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model's Params:  {'learning_decay': 0.9, 'n_components': 10}\n",
      "Best Log Likelihood Score:  -1801093.800076484\n",
      "Model Perplexity:  6663.630677256372\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "model = GridSearchLDA(dtm)\n",
    "\n",
    "# Best Model\n",
    "best_lda_model = model.best_estimator_\n",
    "\n",
    "# Model Parameters\n",
    "print(\"Best Model's Params: \", model.best_params_)\n",
    "\n",
    "# Log Likelihood Score\n",
    "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
    "\n",
    "# Perplexity\n",
    "print(\"Model Perplexity: \", best_lda_model.perplexity(dtm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top n keywords for each topic\n",
    "def show_topics(vectorizer=vectorizer, lda_model=lda_model, n_words=20):\n",
    "    keywords = np.array(vectorizer.get_feature_names())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    return topic_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 0</th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Word 3</th>\n",
       "      <th>Word 4</th>\n",
       "      <th>Word 5</th>\n",
       "      <th>Word 6</th>\n",
       "      <th>Word 7</th>\n",
       "      <th>Word 8</th>\n",
       "      <th>Word 9</th>\n",
       "      <th>Word 10</th>\n",
       "      <th>Word 11</th>\n",
       "      <th>Word 12</th>\n",
       "      <th>Word 13</th>\n",
       "      <th>Word 14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic 0</th>\n",
       "      <td>beatles</td>\n",
       "      <td>gorilla</td>\n",
       "      <td>ritz</td>\n",
       "      <td>korman</td>\n",
       "      <td>champ</td>\n",
       "      <td>paul</td>\n",
       "      <td>brothers</td>\n",
       "      <td>dink</td>\n",
       "      <td>lugosi</td>\n",
       "      <td>beery</td>\n",
       "      <td>stevens</td>\n",
       "      <td>hogan</td>\n",
       "      <td>bela</td>\n",
       "      <td>ringo</td>\n",
       "      <td>kelly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 1</th>\n",
       "      <td>henry</td>\n",
       "      <td>holmes</td>\n",
       "      <td>june</td>\n",
       "      <td>anais</td>\n",
       "      <td>miller</td>\n",
       "      <td>sherlock</td>\n",
       "      <td>clare</td>\n",
       "      <td>washington</td>\n",
       "      <td>nin</td>\n",
       "      <td>erotic</td>\n",
       "      <td>bana</td>\n",
       "      <td>traveler</td>\n",
       "      <td>watson</td>\n",
       "      <td>romanian</td>\n",
       "      <td>thurman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 2</th>\n",
       "      <td>douglas</td>\n",
       "      <td>powell</td>\n",
       "      <td>pesci</td>\n",
       "      <td>vinny</td>\n",
       "      <td>shields</td>\n",
       "      <td>chuck</td>\n",
       "      <td>fred</td>\n",
       "      <td>moustache</td>\n",
       "      <td>columbo</td>\n",
       "      <td>jonathan</td>\n",
       "      <td>tomei</td>\n",
       "      <td>kirk</td>\n",
       "      <td>norris</td>\n",
       "      <td>cousin</td>\n",
       "      <td>dick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 3</th>\n",
       "      <td>rogers</td>\n",
       "      <td>egyptian</td>\n",
       "      <td>emil</td>\n",
       "      <td>ginger</td>\n",
       "      <td>valentino</td>\n",
       "      <td>match</td>\n",
       "      <td>garner</td>\n",
       "      <td>ferrari</td>\n",
       "      <td>deaf</td>\n",
       "      <td>gavras</td>\n",
       "      <td>johnson</td>\n",
       "      <td>sheik</td>\n",
       "      <td>dove</td>\n",
       "      <td>stooges</td>\n",
       "      <td>lonesome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 4</th>\n",
       "      <td>lee</td>\n",
       "      <td>yee</td>\n",
       "      <td>kitano</td>\n",
       "      <td>station</td>\n",
       "      <td>tony</td>\n",
       "      <td>lust</td>\n",
       "      <td>sisko</td>\n",
       "      <td>caution</td>\n",
       "      <td>ember</td>\n",
       "      <td>ang</td>\n",
       "      <td>japanese</td>\n",
       "      <td>tang</td>\n",
       "      <td>leung</td>\n",
       "      <td>wang</td>\n",
       "      <td>mayor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 5</th>\n",
       "      <td>sharpe</td>\n",
       "      <td>lenzi</td>\n",
       "      <td>acerola</td>\n",
       "      <td>laranjinha</td>\n",
       "      <td>vood</td>\n",
       "      <td>ajith</td>\n",
       "      <td>vishnu</td>\n",
       "      <td>betti</td>\n",
       "      <td>silva</td>\n",
       "      <td>naples</td>\n",
       "      <td>varalaru</td>\n",
       "      <td>ace</td>\n",
       "      <td>cunha</td>\n",
       "      <td>ela</td>\n",
       "      <td>rifle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 6</th>\n",
       "      <td>beast</td>\n",
       "      <td>disney</td>\n",
       "      <td>belle</td>\n",
       "      <td>fly</td>\n",
       "      <td>beauty</td>\n",
       "      <td>seth</td>\n",
       "      <td>goldblum</td>\n",
       "      <td>cronenberg</td>\n",
       "      <td>hercules</td>\n",
       "      <td>animated</td>\n",
       "      <td>brundle</td>\n",
       "      <td>castle</td>\n",
       "      <td>gaston</td>\n",
       "      <td>maurice</td>\n",
       "      <td>davis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 7</th>\n",
       "      <td>romero</td>\n",
       "      <td>political</td>\n",
       "      <td>flynn</td>\n",
       "      <td>chong</td>\n",
       "      <td>annabel</td>\n",
       "      <td>salvador</td>\n",
       "      <td>potter</td>\n",
       "      <td>argentina</td>\n",
       "      <td>archbishop</td>\n",
       "      <td>arnaz</td>\n",
       "      <td>teachers</td>\n",
       "      <td>fiennes</td>\n",
       "      <td>monica</td>\n",
       "      <td>roderick</td>\n",
       "      <td>gamera</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 8</th>\n",
       "      <td>time</td>\n",
       "      <td>story</td>\n",
       "      <td>like</td>\n",
       "      <td>people</td>\n",
       "      <td>just</td>\n",
       "      <td>great</td>\n",
       "      <td>love</td>\n",
       "      <td>really</td>\n",
       "      <td>life</td>\n",
       "      <td>characters</td>\n",
       "      <td>character</td>\n",
       "      <td>best</td>\n",
       "      <td>good</td>\n",
       "      <td>films</td>\n",
       "      <td>family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 9</th>\n",
       "      <td>like</td>\n",
       "      <td>good</td>\n",
       "      <td>just</td>\n",
       "      <td>time</td>\n",
       "      <td>really</td>\n",
       "      <td>story</td>\n",
       "      <td>great</td>\n",
       "      <td>make</td>\n",
       "      <td>don</td>\n",
       "      <td>bad</td>\n",
       "      <td>way</td>\n",
       "      <td>movies</td>\n",
       "      <td>plot</td>\n",
       "      <td>people</td>\n",
       "      <td>does</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Word 0     Word 1   Word 2      Word 3     Word 4    Word 5  \\\n",
       "Topic 0  beatles    gorilla     ritz      korman      champ      paul   \n",
       "Topic 1    henry     holmes     june       anais     miller  sherlock   \n",
       "Topic 2  douglas     powell    pesci       vinny    shields     chuck   \n",
       "Topic 3   rogers   egyptian     emil      ginger  valentino     match   \n",
       "Topic 4      lee        yee   kitano     station       tony      lust   \n",
       "Topic 5   sharpe      lenzi  acerola  laranjinha       vood     ajith   \n",
       "Topic 6    beast     disney    belle         fly     beauty      seth   \n",
       "Topic 7   romero  political    flynn       chong    annabel  salvador   \n",
       "Topic 8     time      story     like      people       just     great   \n",
       "Topic 9     like       good     just        time     really     story   \n",
       "\n",
       "           Word 6      Word 7      Word 8      Word 9    Word 10   Word 11  \\\n",
       "Topic 0  brothers        dink      lugosi       beery    stevens     hogan   \n",
       "Topic 1     clare  washington         nin      erotic       bana  traveler   \n",
       "Topic 2      fred   moustache     columbo    jonathan      tomei      kirk   \n",
       "Topic 3    garner     ferrari        deaf      gavras    johnson     sheik   \n",
       "Topic 4     sisko     caution       ember         ang   japanese      tang   \n",
       "Topic 5    vishnu       betti       silva      naples   varalaru       ace   \n",
       "Topic 6  goldblum  cronenberg    hercules    animated    brundle    castle   \n",
       "Topic 7    potter   argentina  archbishop       arnaz   teachers   fiennes   \n",
       "Topic 8      love      really        life  characters  character      best   \n",
       "Topic 9     great        make         don         bad        way    movies   \n",
       "\n",
       "        Word 12   Word 13   Word 14  \n",
       "Topic 0    bela     ringo     kelly  \n",
       "Topic 1  watson  romanian   thurman  \n",
       "Topic 2  norris    cousin      dick  \n",
       "Topic 3    dove   stooges  lonesome  \n",
       "Topic 4   leung      wang     mayor  \n",
       "Topic 5   cunha       ela     rifle  \n",
       "Topic 6  gaston   maurice     davis  \n",
       "Topic 7  monica  roderick    gamera  \n",
       "Topic 8    good     films    family  \n",
       "Topic 9    plot    people      does  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_keywords = show_topics(vectorizer=vectorizer, lda_model=best_lda_model, n_words=15)        \n",
    "\n",
    "# Topic - Keywords Dataframe\n",
    "df_topic_keywords = pd.DataFrame(topic_keywords)\n",
    "df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\n",
    "df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\n",
    "df_topic_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Interpreting Topic Modeling output__\n",
    " - Looking at the top 15 words per topic, it closely resembles genres\n",
    "  - Topic 0 - Song titles/Artistes\n",
    "  - Topic 1 - Thriller Action Movies\n",
    "  - Topic 4 - Asian Movies\n",
    "  - The rest wasnt clear to me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize your bags\n",
    "In the above exercise, you may find it important to normalize your data.  One useful method when dealing with text is *Term Frequency - Inverse Document Frequency (TF-IDF)*. You can see more detail on this here: http://blog.christianperone.com/2011/10/machine-learning-text-feature-extraction-tf-idf-part-ii/.\n",
    "\n",
    "Once you understand the concept, **express your data as TF-IDF vectors (instead of simple bag-of-words counts), and see if it changes your above story**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_tfid = TfidfTransformer().fit_transform(dtm)\n",
    "dtm_tfid = dtm_tfid.toarray() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaah</th>\n",
       "      <th>aahed</th>\n",
       "      <th>aames</th>\n",
       "      <th>aaron</th>\n",
       "      <th>aazmi</th>\n",
       "      <th>aback</th>\n",
       "      <th>abaddon</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandoning</th>\n",
       "      <th>abandonment</th>\n",
       "      <th>abandons</th>\n",
       "      <th>abaskharon</th>\n",
       "      <th>abattoir</th>\n",
       "      <th>abbas</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abbie</th>\n",
       "      <th>abbott</th>\n",
       "      <th>abbreviated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   aaa  aaah  aahed  aames  aaron  aazmi  aback  abaddon  abandon  abandoned  \\\n",
       "0  0.0   0.0    0.0    0.0    0.0    0.0    0.0      0.0      0.0        0.0   \n",
       "1  0.0   0.0    0.0    0.0    0.0    0.0    0.0      0.0      0.0        0.0   \n",
       "2  0.0   0.0    0.0    0.0    0.0    0.0    0.0      0.0      0.0        0.0   \n",
       "3  0.0   0.0    0.0    0.0    0.0    0.0    0.0      0.0      0.0        0.0   \n",
       "4  0.0   0.0    0.0    0.0    0.0    0.0    0.0      0.0      0.0        0.0   \n",
       "\n",
       "   abandoning  abandonment  abandons  abaskharon  abattoir  abbas  abbey  \\\n",
       "0         0.0          0.0       0.0         0.0       0.0    0.0    0.0   \n",
       "1         0.0          0.0       0.0         0.0       0.0    0.0    0.0   \n",
       "2         0.0          0.0       0.0         0.0       0.0    0.0    0.0   \n",
       "3         0.0          0.0       0.0         0.0       0.0    0.0    0.0   \n",
       "4         0.0          0.0       0.0         0.0       0.0    0.0    0.0   \n",
       "\n",
       "   abbie  abbott  abbreviated  \n",
       "0    0.0     0.0          0.0  \n",
       "1    0.0     0.0          0.0  \n",
       "2    0.0     0.0          0.0  \n",
       "3    0.0     0.0          0.0  \n",
       "4    0.0     0.0          0.0  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame(dtm_tfid, columns=vocab)\n",
    "df[vocab[0:20]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent word: like\n"
     ]
    }
   ],
   "source": [
    "getFrequentWord(dtm_tfid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model's Params:  {'learning_decay': 0.9, 'n_components': 10}\n",
      "Best Log Likelihood Score:  -175736.99747983975\n",
      "Model Perplexity:  9016.576452889705\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "model = GridSearchLDA(dtm_tfid)\n",
    "\n",
    "# Best Model\n",
    "best_lda_model = model.best_estimator_\n",
    "\n",
    "# Model Parameters\n",
    "print(\"Best Model's Params: \", model.best_params_)\n",
    "\n",
    "# Log Likelihood Score\n",
    "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
    "\n",
    "# Perplexity\n",
    "print(\"Model Perplexity: \", best_lda_model.perplexity(dtm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 0</th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Word 3</th>\n",
       "      <th>Word 4</th>\n",
       "      <th>Word 5</th>\n",
       "      <th>Word 6</th>\n",
       "      <th>Word 7</th>\n",
       "      <th>Word 8</th>\n",
       "      <th>Word 9</th>\n",
       "      <th>Word 10</th>\n",
       "      <th>Word 11</th>\n",
       "      <th>Word 12</th>\n",
       "      <th>Word 13</th>\n",
       "      <th>Word 14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic 0</th>\n",
       "      <td>laali</td>\n",
       "      <td>priya</td>\n",
       "      <td>namaga</td>\n",
       "      <td>blokey</td>\n",
       "      <td>tvand</td>\n",
       "      <td>larrikin</td>\n",
       "      <td>siam</td>\n",
       "      <td>buffoonery</td>\n",
       "      <td>batman</td>\n",
       "      <td>drake</td>\n",
       "      <td>writing</td>\n",
       "      <td>backfire</td>\n",
       "      <td>friels</td>\n",
       "      <td>just</td>\n",
       "      <td>way</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 1</th>\n",
       "      <td>neutrality</td>\n",
       "      <td>tryed</td>\n",
       "      <td>edeid</td>\n",
       "      <td>zaky</td>\n",
       "      <td>gendy</td>\n",
       "      <td>nabeela</td>\n",
       "      <td>micol</td>\n",
       "      <td>morrow</td>\n",
       "      <td>feldman</td>\n",
       "      <td>marcos</td>\n",
       "      <td>time</td>\n",
       "      <td>georgio</td>\n",
       "      <td>cobbs</td>\n",
       "      <td>alex</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 2</th>\n",
       "      <td>colombo</td>\n",
       "      <td>diagnosis</td>\n",
       "      <td>racketeers</td>\n",
       "      <td>againist</td>\n",
       "      <td>shotten</td>\n",
       "      <td>incommunicability</td>\n",
       "      <td>sabbaththemovie</td>\n",
       "      <td>decreased</td>\n",
       "      <td>digressions</td>\n",
       "      <td>livid</td>\n",
       "      <td>speckle</td>\n",
       "      <td>andelman</td>\n",
       "      <td>dubiety</td>\n",
       "      <td>flutters</td>\n",
       "      <td>earnestly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 3</th>\n",
       "      <td>appetizing</td>\n",
       "      <td>suburbanite</td>\n",
       "      <td>brightens</td>\n",
       "      <td>coz</td>\n",
       "      <td>inhaling</td>\n",
       "      <td>lauderdale</td>\n",
       "      <td>appetit</td>\n",
       "      <td>longstocking</td>\n",
       "      <td>panflute</td>\n",
       "      <td>cheesiest</td>\n",
       "      <td>pippi</td>\n",
       "      <td>wallower</td>\n",
       "      <td>dragomir</td>\n",
       "      <td>psychodynamics</td>\n",
       "      <td>walrus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 4</th>\n",
       "      <td>nesmith</td>\n",
       "      <td>ottola</td>\n",
       "      <td>blooper</td>\n",
       "      <td>topcoat</td>\n",
       "      <td>tufted</td>\n",
       "      <td>stucco</td>\n",
       "      <td>lancelot</td>\n",
       "      <td>mademoiselle</td>\n",
       "      <td>portals</td>\n",
       "      <td>accrues</td>\n",
       "      <td>inexexplicably</td>\n",
       "      <td>aestheticians</td>\n",
       "      <td>maple</td>\n",
       "      <td>aficianados</td>\n",
       "      <td>unpainted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 5</th>\n",
       "      <td>justin</td>\n",
       "      <td>kanew</td>\n",
       "      <td>congo</td>\n",
       "      <td>cosovo</td>\n",
       "      <td>wether</td>\n",
       "      <td>bosnia</td>\n",
       "      <td>economical</td>\n",
       "      <td>albania</td>\n",
       "      <td>solanas</td>\n",
       "      <td>institutional</td>\n",
       "      <td>argentinian</td>\n",
       "      <td>brault</td>\n",
       "      <td>montr</td>\n",
       "      <td>nastassja</td>\n",
       "      <td>canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 6</th>\n",
       "      <td>mulroney</td>\n",
       "      <td>primed</td>\n",
       "      <td>dissects</td>\n",
       "      <td>reassess</td>\n",
       "      <td>dermor</td>\n",
       "      <td>maris</td>\n",
       "      <td>garner</td>\n",
       "      <td>peggy</td>\n",
       "      <td>play</td>\n",
       "      <td>just</td>\n",
       "      <td>fox</td>\n",
       "      <td>love</td>\n",
       "      <td>sunday</td>\n",
       "      <td>ann</td>\n",
       "      <td>like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 7</th>\n",
       "      <td>platforms</td>\n",
       "      <td>enfield</td>\n",
       "      <td>spanking</td>\n",
       "      <td>adrianna</td>\n",
       "      <td>sdfm</td>\n",
       "      <td>zentradi</td>\n",
       "      <td>hancock</td>\n",
       "      <td>kawamori</td>\n",
       "      <td>swart</td>\n",
       "      <td>dialoge</td>\n",
       "      <td>diver</td>\n",
       "      <td>soave</td>\n",
       "      <td>programa</td>\n",
       "      <td>lilja</td>\n",
       "      <td>anjos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 8</th>\n",
       "      <td>like</td>\n",
       "      <td>good</td>\n",
       "      <td>just</td>\n",
       "      <td>great</td>\n",
       "      <td>story</td>\n",
       "      <td>really</td>\n",
       "      <td>time</td>\n",
       "      <td>people</td>\n",
       "      <td>movies</td>\n",
       "      <td>don</td>\n",
       "      <td>love</td>\n",
       "      <td>watch</td>\n",
       "      <td>think</td>\n",
       "      <td>best</td>\n",
       "      <td>films</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 9</th>\n",
       "      <td>aparent</td>\n",
       "      <td>mcgurney</td>\n",
       "      <td>storylines</td>\n",
       "      <td>angelle</td>\n",
       "      <td>aparante</td>\n",
       "      <td>commanders</td>\n",
       "      <td>miscreant</td>\n",
       "      <td>motivo</td>\n",
       "      <td>incisive</td>\n",
       "      <td>leukaemia</td>\n",
       "      <td>commandment</td>\n",
       "      <td>pawnee</td>\n",
       "      <td>paricularly</td>\n",
       "      <td>pinkett</td>\n",
       "      <td>jada</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Word 0       Word 1      Word 2    Word 3    Word 4  \\\n",
       "Topic 0       laali        priya      namaga    blokey     tvand   \n",
       "Topic 1  neutrality        tryed       edeid      zaky     gendy   \n",
       "Topic 2     colombo    diagnosis  racketeers  againist   shotten   \n",
       "Topic 3  appetizing  suburbanite   brightens       coz  inhaling   \n",
       "Topic 4     nesmith       ottola     blooper   topcoat    tufted   \n",
       "Topic 5      justin        kanew       congo    cosovo    wether   \n",
       "Topic 6    mulroney       primed    dissects  reassess    dermor   \n",
       "Topic 7   platforms      enfield    spanking  adrianna      sdfm   \n",
       "Topic 8        like         good        just     great     story   \n",
       "Topic 9     aparent     mcgurney  storylines   angelle  aparante   \n",
       "\n",
       "                    Word 5           Word 6        Word 7       Word 8  \\\n",
       "Topic 0           larrikin             siam    buffoonery       batman   \n",
       "Topic 1            nabeela            micol        morrow      feldman   \n",
       "Topic 2  incommunicability  sabbaththemovie     decreased  digressions   \n",
       "Topic 3         lauderdale          appetit  longstocking     panflute   \n",
       "Topic 4             stucco         lancelot  mademoiselle      portals   \n",
       "Topic 5             bosnia       economical       albania      solanas   \n",
       "Topic 6              maris           garner         peggy         play   \n",
       "Topic 7           zentradi          hancock      kawamori        swart   \n",
       "Topic 8             really             time        people       movies   \n",
       "Topic 9         commanders        miscreant        motivo     incisive   \n",
       "\n",
       "                Word 9         Word 10        Word 11      Word 12  \\\n",
       "Topic 0          drake         writing       backfire       friels   \n",
       "Topic 1         marcos            time        georgio        cobbs   \n",
       "Topic 2          livid         speckle       andelman      dubiety   \n",
       "Topic 3      cheesiest           pippi       wallower     dragomir   \n",
       "Topic 4        accrues  inexexplicably  aestheticians        maple   \n",
       "Topic 5  institutional     argentinian         brault        montr   \n",
       "Topic 6           just             fox           love       sunday   \n",
       "Topic 7        dialoge           diver          soave     programa   \n",
       "Topic 8            don            love          watch        think   \n",
       "Topic 9      leukaemia     commandment         pawnee  paricularly   \n",
       "\n",
       "                Word 13    Word 14  \n",
       "Topic 0            just        way  \n",
       "Topic 1            alex       good  \n",
       "Topic 2        flutters  earnestly  \n",
       "Topic 3  psychodynamics     walrus  \n",
       "Topic 4     aficianados  unpainted  \n",
       "Topic 5       nastassja     canada  \n",
       "Topic 6             ann       like  \n",
       "Topic 7           lilja      anjos  \n",
       "Topic 8            best      films  \n",
       "Topic 9         pinkett       jada  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_keywords = show_topics(vectorizer=vectorizer, lda_model=best_lda_model, n_words=15)        \n",
    "\n",
    "# Topic - Keywords Dataframe\n",
    "df_topic_keywords = pd.DataFrame(topic_keywords)\n",
    "df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\n",
    "df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\n",
    "df_topic_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show us your bags (Version 2)\n",
    "\n",
    "Show and explain what one of your documents looks like as a TF-IDF vector below.  How is this different from a simple bag-of-words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Explain the differences after running this in the cluster__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Simple Supervised Learning with Text\n",
    "Now that you are comfortable with treating text as numbers, we can try out supervised learning.  We'll use a labelled dataset of IMDB reviews to classify each review as 'positive' or 'negative'.  You can **find the data below:**\n",
    "\n",
    "http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "\n",
    "Load in and process the data, then train a supervised learning model.  **You should achieve val or test set accuracy of 85%**. Pretty good for a simple bag, no?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in train data: 25000\n",
      "Samples per class (train): [12500 12500]\n",
      "Number of documents in test data: 25000\n",
      "Samples per class (test): [12500 12500]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "reviews_train = load_files(\"aclImdb/train/\")\n",
    "text_train, y_train = reviews_train.data, reviews_train.target\n",
    "\n",
    "print(\"Number of documents in train data: {}\".format(len(text_train)))\n",
    "print(\"Samples per class (train): {}\".format(np.bincount(y_train)))\n",
    "\n",
    "reviews_test = load_files(\"aclImdb/test/\")\n",
    "text_test, y_test = reviews_test.data, reviews_test.target\n",
    "\n",
    "print(\"Number of documents in test data: {}\".format(len(text_test)))\n",
    "print(\"Samples per class (test): {}\".format(np.bincount(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 129549\n",
      "X_train:\n",
      "<25000x129549 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 3607330 stored elements in Compressed Sparse Row format>\n",
      "X_test: \n",
      "<25000x129549 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 3392376 stored elements in Compressed Sparse Row format>\n",
      "Number of features: 129549\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer(min_df=5, ngram_range=(2, 2))\n",
    "X_train = vect.fit(text_train).transform(text_train)\n",
    "X_test = vect.transform(text_test)\n",
    "\n",
    "print(\"Vocabulary size: {}\".format(len(vect.vocabulary_)))\n",
    "print(\"X_train:\\n{}\".format(repr(X_train)))\n",
    "print(\"X_test: \\n{}\".format(repr(X_test)))\n",
    "\n",
    "feature_names = vect.get_feature_names()\n",
    "print(\"Number of features: {}\".format(len(feature_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hanifa/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.88\n",
      "Best parameters:  {'C': 1}\n",
      "Best estimator:  LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
      "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
      "          tol=0.0001, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\n",
    "grid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Best parameters: \", grid.best_params_)\n",
    "print(\"Best estimator: \", grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hanifa/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.88\n"
     ]
    }
   ],
   "source": [
    "lr = grid.best_estimator_\n",
    "lr.fit(X_train, y_train)\n",
    "lr.predict(X_test)\n",
    "print(\"Score: {:.2f}\".format(lr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Playing with Recurrent Neural Networks (RNN)\n",
    "So far, we've only treated text as a simple bag, with reasonable results.  We'll now shift to a more complex representation of language: recurrent neural networks.  To do so, we need to process text at the word or character level, and capture the sequence of a document. \n",
    "\n",
    "Our task here is to build an RNN that 'eats up' sequences of characters in order to predict the next character in a sequence, for every step in the sequence of a document. This is a common (and fun) task, with lots of examples available online. \n",
    "\n",
    "For this task, use existing RNN APIs (don't code everything from scratch) from Keras or PyTorch. \n",
    "\n",
    "**Read up on RNNs and this exercise** below:\n",
    "* http://karpathy.github.io/2015/05/21/rnn-effectiveness/ - start here!\n",
    "* https://github.com/martin-gorner/tensorflow-rnn-shakespeare - video, slides and code going through an example with Shakespeare\n",
    "* http://killianlevacher.github.io/blog/posts/post-2016-03-01/post.html - another nice example based on Trump tweets\n",
    "\n",
    "### Prepare your data\n",
    "\n",
    "Our first step is to prepare our text. **Process your corpora into a format that can be used by an RNN, and walkthough one sequence below**.\n",
    "\n",
    "An **example way to shape your data** for this task is as follows (feel free to play around with different structures):\n",
    "\n",
    "*In this example your corpora starts with the string 'the cat and I'*\n",
    "* RNN input: divide your text into sequences of 10 characters e.g. 'the cat an'\n",
    "* RNN output: the 1 character immediately following RNN input sequences e.g. 'd'. \n",
    "* Note: You may or may not want to divide your text into overlapping strings (e.g. RNN input contains 'the cat an', 'he cat and', 'e cat and ', ...) . How is the model different in each case?\n",
    "* Note: Your 'vocabulary' or `vocab_size` here is the number of unique characters in your text (and therefore the number of classes you want to predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/nietzsche.txt\n",
      "606208/600901 [==============================] - 0s 1us/step\n",
      "Corpus length: 600893\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "path = keras.utils.get_file(\n",
    "    'nietzsche.txt',\n",
    "    origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "text = open(path).read().lower()\n",
    "print('Corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences: 200278\n",
      "Unique characters: 57\n",
      "Vectorization...\n"
     ]
    }
   ],
   "source": [
    "# Length of extracted character sequences\n",
    "maxlen = 60\n",
    "\n",
    "# We sample a new sequence every `step` characters\n",
    "step = 3\n",
    "\n",
    "# This holds our extracted sequences\n",
    "sentences = []\n",
    "\n",
    "# This holds the targets (the follow-up characters)\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('Number of sequences:', len(sentences))\n",
    "\n",
    "# List of unique characters in the corpus\n",
    "chars = sorted(list(set(text)))\n",
    "print('Unique characters:', len(chars))\n",
    "# Dictionary mapping unique characters to their index in `chars`\n",
    "char_indices = dict((char, chars.index(char)) for char in chars)\n",
    "\n",
    "# Next, one-hot encode the characters into binary arrays.\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(layers.Dense(len(chars), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 154s 770us/step - loss: 1.9992\n",
      "--- Generating with seed: \"ecome\n",
      "strong enough, hard enough, artist enough.... piety, t\"\n",
      "------ temperature: 0.2\n",
      "ecome\n",
      "strong enough, hard enough, artist enough.... piety, the streng to the such a something and schopening to the world despaint to the such a present to the world and streally and schoues to the consideration and consideral and consideration of the such a soletion of the something to the exteration of the consideration of the some of the such a something the world in the pressions of the streng to the may to the such a despression of the soul and the co\n",
      "------ temperature: 0.5\n",
      " to the may to the such a despression of the soul and the consistains to the a religious to his hand the exament of the way streng and the sous simpling the german the means to be existiment to the indection of comparence for the saims of the would consernal far the sechily in the a putainty of the seeter all the say to be distance of the man is a was to who would which the expection to the such all the world sometomes and the some of the mays all the one \n",
      "------ temperature: 1.0\n",
      "ll the world sometomes and the some of the mays all the one its and somgthit uslends it, succ an this\n",
      "to soe, to the worldul ye world e is as bediring betimeness or hdiscuress worditicatms to them, the usensable of diterfience, breness in religial thery espreation to may break gradings the\n",
      "gedenifer, world alfiegolied to you deever euisthics. the prope, \n",
      "hreever is to : we scherouns and most gorsibli mord a their somening to why a usoups of threak (is lear\n",
      "------ temperature: 1.2\n",
      "bli mord a their somening to why a usoups of threak (is lear in 1utty som2tencesneing\n",
      "for diskbrt\" to caye.\n",
      "\n",
      "\n",
      "a37hil su have kenowwess attiniex\n",
      "womling essedled lived this demonomine.\n",
      "wom, for and cooincus is a toplually leal, vritise od mosh\n",
      "decer among and duvides scientultation ward.\n",
      "whatger of ciacestnot, with threefsistlyol.\n",
      "by\n",
      "inugents and the ,utheru, god how is otherw cound lahity dry \"itsler sceem, tobituribess(s of and itmislod\n",
      "one geaks and scho\n",
      "epoch 2\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 139s 693us/step - loss: 1.6467\n",
      "--- Generating with seed: \"hey rank the credibility of their own bodies about as low as\"\n",
      "------ temperature: 0.2\n",
      "hey rank the credibility of their own bodies about as low as the wast and the man and the compresent the all the seeming the conservations and the profound the interpretions of the self which is the self there is the self in the self and the man that the self the such and that the world of the world and the world of the fall the wast that the self and the self and the man that the senses that the self standing the self in the self in the man in the complet\n",
      "------ temperature: 0.5\n",
      "self standing the self in the self in the man in the completers to a shart of greates and the self experience and the man which wart will be all such the conserval greates with the words and interpresent the seems the least the cormalled in the doen, the promosed the posses from the wertain experient in the master compreasions, the such a sense and man that the truths what refarrity, and is the means of such the inversations of will inclusion that a respec\n",
      "------ temperature: 1.0\n",
      "ans of such the inversations of will inclusion that a respectarty, weffury. as well the impartiinal form. however at fore not strange of\n",
      "promatione, who german, indicernary\n",
      "eyes, araquaity honest to grenore ecuine bre aloun that had\n",
      "poodnging, in theme-is as\n",
      "averoty. them, what bebace.t the which butde with reflive stich haritor that,-and more immenderly\n",
      "we certuisism of valuing, to stept religion in this axplageration from poetirely be of new inse\"furity \n",
      "------ temperature: 1.2\n",
      " in this axplageration from poetirely be of new inse\"furity of quiver of litely , say, words, oraree with utily at indeed with thar urreading and speries\n",
      "for doublest.ful and ideareros's quarity of was no medior first me doingity has\n",
      "them varty franmate europethbor, varilary deluse of more doe still\" alther so arguping follund. of other.\n",
      "idpercainty as\" bads tender so3\" may\n",
      "everathise madecirfling wo, -full\n",
      "pountion,\n",
      "\n",
      "brolacs, aideing which rab4uc. the jes\n",
      "epoch 3\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 140s 699us/step - loss: 1.5558\n",
      "--- Generating with seed: \" bow was bent!\n",
      "     strongest was he by whom such bolt were \"\n",
      "------ temperature: 0.2\n",
      " bow was bent!\n",
      "     strongest was he by whom such bolt were the sense of the sense of the contraint the sense of the sense of the sense of the sense of the there is not the sense and there is not the sense of the experition of the sense the sense of the sense the will and the sense of the the sense of the sense and there is not the sense of the sense of the sense of the sense them the sense of the present the more present the experience of the sense of the\n",
      "------ temperature: 0.5\n",
      " present the more present the experience of the sense of the in many religious for the the faith--and there is not them there is not the inceptions of them one the persesply there is not the which the experitions and decided by the seemed something is the secuility what one which is the element and sensed to despress and there is still cannot the present, and in the many the many in an any the sense of them not in the mensus the circt; he work and conditio\n",
      "------ temperature: 1.0\n",
      "se of them not in the mensus the circt; he work and condition\" so sen to the whole\n",
      "its womdiain of all a it divints of evolut of expetien expression them thenser the family, them pleas over in\n",
      "mydenised in its decided framen there\n",
      "in the perciptimism in thereinds through the generally are praise the peale tastes it, as alturnd point 1with arroughts, has one expecdful, the\n",
      "higher falt\n",
      "of may called labficiunable. which experiction; and any us on disgre.\n",
      "\n",
      "lo\n",
      "------ temperature: 1.2\n",
      " labficiunable. which experiction; and any us on disgre.\n",
      "\n",
      "lotes an lasr axtend t\n",
      "st\n",
      "all aricte opinion nately.\n",
      "\n",
      "hrar herged moral,\n",
      "unciets of instinct\n",
      "which a bevolose assolanical unitank as an itdenesty.=--or was\n",
      "baver: the  toni1faria.\n",
      "curhocr expressing oftancer.\n",
      "\n",
      "lest, then advictly dictione coevilual\n",
      "mens of ever\n",
      "ofdes as.\" him\n",
      "rad the fun of afferst experiotty.\"--art with them as was--thincs hending last\n",
      "travilety vitue yead\n",
      "goody to trow--.\n",
      "for in t\n",
      "epoch 4\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 139s 695us/step - loss: 1.5078\n",
      "--- Generating with seed: \" determine worth and rank according to the amount and variet\"\n",
      "------ temperature: 0.2\n",
      " determine worth and rank according to the amount and varietical such and the religious and the soul is the same the same the sense of the soul and the soul and the stand of the surpons of the deligious and the soul as the sense of the soul and the soul and the soul and the same the restrans the same the restraction of the sense of the same the same the sense of the most the most presence of the soul is the same the more that is the demative of the same th\n",
      "------ temperature: 0.5\n",
      "oul is the same the more that is the demative of the same the sensions of the same that which that is the gods to stim it is the most consequence then that is the araint to become the parance of the really as a such our wearing that which is well and that some deed the senses of morality. when pensible of the more and ore to be morality is not have the good say that is not be not are womle predimitary himself--the most be do there they in the more themsent\n",
      "------ temperature: 1.0\n",
      "tary himself--the most be do there they in the more themsent the\n",
      "deductually a coveraccle!\n",
      "-withtal\n",
      "assome that of strugging\n",
      "mess\n",
      "sable\n",
      "some pirty. without the point therefore by his christion of hrouch of its be pativating, ask\n",
      "every\n",
      "drea, but the\n",
      "possible, as a famire we blos. of litting they not of delusion may permean are philosoms. they a fear: a thought oransion. by nogenessing \"wanoul are had devolince at the world and\n",
      "inredound a whole like that re\n",
      "------ temperature: 1.2\n",
      "ad devolince at the world and\n",
      "inredound a whole like that renaal, (certlemeting dey from philosops epeot that of mu hiself ocurievs us our was this are, its but and dutit. indiardiest everyblah\n",
      "viirs and industrable dles true imseverer brung\".\n",
      "\n",
      "\n",
      "   cereaint: chacted he\n",
      "well  pessictisousm enturesbic\n",
      "yeals? nua parinageim withoutelas, hence. san:\n",
      "stopeges.          feel dilend, to timaging schrushed cure co labtay, and leadow iromitis. the manr-evour!\n",
      "\"best\n",
      "epoch 5\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 139s 696us/step - loss: 1.4762\n",
      "--- Generating with seed: \"he past or of the foreign is tested,\n",
      "put on, taken off, pack\"\n",
      "------ temperature: 0.2\n",
      "he past or of the foreign is tested,\n",
      "put on, taken off, pack of the conceals of the intellection of the strength that the spirit than the problem and conceals of the sense of the disposition of the worth the spirit and the sense of the most problem of the sense of the way that the way that the concerning the spirit of the sense that the strength the way of the same the sense of the spirit and contempt of the way that the soul, that the sense of the individ\n",
      "------ temperature: 0.5\n",
      "empt of the way that the soul, that the sense of the individual, the still all perhaps nature\n",
      "to the free spirit of the greated distinctive of the whole that of the great of philosophy have such all as a delured and most emotion of the still the\n",
      "and the unstraction,\n",
      "in the foreulthered, and preserved and provable science and the self belief and thing could problem of the bads, the community h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in log\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as in the proticeness of themself and philosophy. that what it is\n",
      "------ temperature: 1.0\n",
      " the proticeness of themself and philosophy. that what it is\n",
      "dispese the cisestoment--as\n",
      "our ages of put the cifemination of\n",
      "philosophy willu infectfoonarious\n",
      "ratherly\n",
      "indispress\n",
      "when betan his olderman vider these mean has intentionalization, command in that, in a rable and, he\n",
      "is no learn bundupousny enjoys, as may for thing-hi. there is his detenration in through among the truth, the masce-hunce\n",
      "of like the ider and granuring and resunards and chille th\n",
      "------ temperature: 1.2\n",
      "e\n",
      "of like the ider and granuring and resunards and chille therefoaten and the dall. novelo\n",
      "atfaccrusum or god,\" to\n",
      "who yet that too and\n",
      "mind the believent of\n",
      "leaihed. over to its value the from the\n",
      "unuponsed distinctivat uponer say,\n",
      "even backhedes,\n",
      "the\n",
      "truth clasm ou\n",
      "opiral\"--as are seurt to may unde\n",
      "all--higs of centartery thereflequality,\n",
      "amten and orient though as laim,  being in all thues love you cause said lights\n",
      "erron us, \n",
      "\n",
      "\n",
      "1\n",
      "pebe which or possible\n",
      "epoch 6\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 140s 699us/step - loss: 1.4559\n",
      "--- Generating with seed: \"es, fruitful and fearful at once, and confronting\n",
      "the world \"\n",
      "------ temperature: 0.2\n",
      "es, fruitful and fearful at once, and confronting\n",
      "the world and in the proper the promise of the same the same the same the more detention of the most the success of the superiority of the same the problem of the most demitic and soul and there is a person the problem of the most the same the most the higher the promise of the most more interest is not be more is a more the spirit to the more the same the spirit and soul and the spirit the superiority of t\n",
      "------ temperature: 0.5\n",
      "same the spirit and soul and the spirit the superiority of the superiority of the more contempore of the spirit to the most be more states of the same they have be unity. there is no reading to the manified. showly with a fact that it is not be really that the german is a same imposed that is a man is the most to the still feels we allough a schopenhauer--think is a more sense in the higsess has may be that in the most to the house and higher and whoel the\n",
      "------ temperature: 1.0\n",
      "ay be that in the most to the house and higher and whoel the even farities wor-deply create spirit of \"mysted for either of the comremplants the shiuley on, perhaps to itself idonged in deplised, liqklent for the time ouc-ricdingic some contemptyint oor expealisters of defens foundaatia\n",
      "prountaded are constranicts of stituden. .  lat original\n",
      "instanceing sacrifies and towards with anything tosities so not in them. that is there  not sharless at the moralis\n",
      "------ temperature: 1.2\n",
      "s so not in them. that is there  not sharless at the moralistes a stream, almost umopsire\" hidsely\n",
      "thee, will\n",
      "is curiosion that us its lighte other. which saprized onliencver of\n",
      "thess. every enoughd foom seccensanimannisad -to the (is every sickerat now nor they nothing what wishously, namilied furthed it is worthingly, world emponispod of the jumbitions.\n",
      "\n",
      "27touns to all mhere light, willy, if io dation and his putray of\n",
      "tymus they\n",
      "ascteds and\n",
      "hwhat all wh\n",
      "epoch 7\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 140s 697us/step - loss: 1.4385\n",
      "--- Generating with seed: \"mething in the german style, manifold, formless, and\n",
      "inexhau\"\n",
      "------ temperature: 0.2\n",
      "mething in the german style, manifold, formless, and\n",
      "inexhaustion of the more to a man the man and self-continue and consequently in the most be distrust of the man and supersponsing and self-continuely in the senses and propers of the most present is the constitutes and sinting himself and supersposing to the sense to the supersposing and prisose is the most present the spirit to the more to the most present the stands and self-continue to the most the mo\n",
      "------ temperature: 0.5\n",
      "most present the stands and self-continue to the most the more become and sense is in the proper solitures, they are the comner there is not be distrection and superiority to the present--it is the important, the most instranges and man and philosophers and carrity of the most in any consider is and consider of the is all the relation of supersposing and seemed to astrory of life, the moral forly affucence of the self-delight may be confidion they not the \n",
      "------ temperature: 1.0\n",
      "affucence of the self-delight may be confidion they not the sert traches imorrement, itself\n",
      "\n",
      "dips feeling which exeresses. they still was that an it is of simply nivious will, in which saccypism,\n",
      "\n",
      "          non with \"expresses, which seemscence in souls the\n",
      "discivite is viction of serpre athain of purpose in the most\n",
      "or strogreable respoesing so faith a signess: become finer that the may coups in such which adventure of shiul: in were existence and lixsing\n",
      "------ temperature: 1.2\n",
      "such which adventure of shiul: in were existence and lixsing time are there is mean, seleks it, who ) trahtance,\n",
      "those, or  \n",
      "    trach to taek. he makes, decirifice,\n",
      "all understand oman andwetrisp: oneselty--they\n",
      "kiss of a good-closp. \"science of presence, ho\n",
      "\"of sum it self-resecriva. ell?\" impussity\n",
      "of peasitude pitlen ideal for lixtening goest soul, a scrrblo? ware patrie, they risest can\n",
      "privernians no life; they need a raned his wornled; dillou plough\n",
      "epoch 8\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 139s 693us/step - loss: 1.4258\n",
      "--- Generating with seed: \" the devil? is everything, in the last\n",
      "resort, false? and if\"\n",
      "------ temperature: 0.2\n",
      " the devil? is everything, in the last\n",
      "resort, false? and if the same an any profound the states of a stands and prives the standing and such an accumution of the stands and protection of the spirit to the spirit of the sense of the succetration of the same of the spirit and science, and stands and succession of the sense the spirit of the spirit of the spirit the earth of the way the spirit of the spirit of the spirit of the sense the spirit and the state\n",
      "------ temperature: 0.5\n",
      "e spirit of the spirit of the sense the spirit and the state of the terally the sage the earth and problem of the man and proper art of such a man has a serious, the hightly of the fact which has own art of every art of the most fraite all in this art and suffered all the still all ideas all and in the higher its of the fact to the instance and priling as a profound present, and respect to discovar and the succetranity, this religious than the determine th\n",
      "------ temperature: 1.0\n",
      "r and the succetranity, this religious than the determine their at they desire and\n",
      "risescs he hado,\" of his, to surrifgrifful titerep of the superspien of night,\n",
      "of by the agan in their ganity of its realizing, literan an excepise, the  consible trayigus--with or believed, they groves labord, larest alave: procerification\n",
      "and inliked of diffadiaus nameling and goranthity of the more, \"indeed of the moral will\n",
      "strist,\n",
      "take cle, and forces, look reflectre,\n",
      "w\n",
      "------ temperature: 1.2\n",
      "e moral will\n",
      "strist,\n",
      "take cle, and forces, look reflectre,\n",
      "without pertial suople, thisip? comorr. the pleasurly,\n",
      "witht. the mand essencedans\"), pholosus with will such utmolt singbe. al\n",
      "interestity aglens\"--alegame. to .\n",
      "much and makes the \n",
      "seassin of the hureageness to does greate, manifests, than sociesctation, with the reaiting, and blar great sel understen iswtitin for immentialedhed di sive in throd, his vigle hypothesis placakingter hand--eassy him \n",
      "epoch 9\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 140s 701us/step - loss: 1.4139\n",
      "--- Generating with seed: \"down to the\n",
      "very animals, up even to \"god\"--the extravagance\"\n",
      "------ temperature: 0.2\n",
      "down to the\n",
      "very animals, up even to \"god\"--the extravagance of the continer of the sense the higher will be the soul and the sense of the most conceine of the world of the sense of the soul and a such the sense of the most belief the sense of the conscience of the sense of the spirit of the such a subtlety of the sense that is the such a soul and the sense of the sensition of the state of the state of the explanation of the world to the state of the sense\n",
      "------ temperature: 0.5\n",
      "te of the explanation of the world to the state of the sense in the general conceases in men and moral instance sacrives a complained by the most man the most present the world even they are depression is new from the will a things the man and in the books, the way that is the stronges and most sufficient idea of the deteriility, and that is the will and fail and as long so long even the feel for him, as he was the most same followage to the hamment and cr\n",
      "------ temperature: 1.0\n",
      "him, as he was the most same followage to the hamment and critus of the knows ware old differently internection wolle\n",
      "whatever a gulance, in litable lightly be\n",
      "disposed. amout lixes and will hard fundamentally the sprake--the fachely, he knows many--and allied the\n",
      "primisrary\n",
      "many that this wayt direct into sou ps\n",
      "least\n",
      "europe, with he generations, who perseed on last--for expected that if the herdern for feat, and hoblets are not a distinguish, is mannesse\n",
      "------ temperature: 1.2\n",
      "ern for feat, and hoblets are not a distinguish, is mannesses as ong ofly peol aid fiding require from demans is the youthments condeing\n",
      "becausen-ganing to my and\n",
      "to them might only accling malsatiot will. les untounralk, intentious enjoy. good cess, and itself,\n",
      "intererre reptance they wising:\n",
      "so deteritigity absure:--nruqupestennessch.buden typach rrestural condition\" new\n",
      "turding, occasioned, so wimad it is morality.\n",
      "thereun intertionce fints to be slowly\n",
      "epoch 10\n",
      "Epoch 1/1\n",
      "200278/200278 [==============================] - 139s 695us/step - loss: 1.4035\n",
      "--- Generating with seed: \"of the emperor tiberius in the\n",
      "mithra-grotto on the island o\"\n",
      "------ temperature: 0.2\n",
      "of the emperor tiberius in the\n",
      "mithra-grotto on the island of the world the present and any present and any property of the successful of the belief the more superiority in the sense of the states of the belief in the experience and in the superity, and any distinguise in the best and for in the experience of the belief the such a conscience and any the present and the experience and case of the same suffering the over the present and also a philosophy of \n",
      "------ temperature: 0.5\n",
      "ame suffering the over the present and also a philosophy of life in the preservation of the has we exist and present and experience and comple as the make the man also be distinguisions and long because be has but the canly the serpoust and the believe is new place of the best between of the\n",
      "experience for its cirits of his present our will to the world, it is a soul and its own believe\n",
      "and history the amisseless of the experience, one understand is not be\n",
      "------ temperature: 1.0\n",
      "y the amisseless of the experience, one understand is not be moduss that also a philosophy is extratation to experience and spites of they look\n",
      "with ritestal willing moded med dingerald, that happly into\n",
      "the conpedded, and voirugy, this tankne of philosopher, pusse, \"alary sencratmings them of con between\n",
      "his ditess--he new\n",
      "rational, but it of the overing caknide. undopwed them owanises clear fuses fears over\n",
      "a fashist, me of the\n",
      "suspicious forcge ferbeate\n",
      "------ temperature: 1.2\n",
      "s fears over\n",
      "a fashist, me of the\n",
      "suspicious forcge ferbeaterty; and \"upmocu of them is ought with hands of love sast, in god, a dispowed \"impulses and these science.f-domoration is itself sreated crable, a meibler.\n",
      "\n",
      "\n",
      "18to quedity, and hrain susposing between the mavilator when if, a schooly--suvanfus randsming.-etims to what moquiths\n",
      "astlyer hind, bitce this whold he capacity. man be \"good and\n",
      "emplustify\n",
      "that opts, ancients everywhularyow.\n",
      "\n",
      "\n",
      "23 f though a\n",
      "epoch 11\n",
      "Epoch 1/1\n",
      " 71808/200278 [=========>....................] - ETA: 1:29 - loss: 1.3762"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import sys\n",
    "\n",
    "for epoch in range(1, 60):\n",
    "    print('epoch', epoch)\n",
    "    # Fit the model for 1 epoch on the available training data\n",
    "    model.fit(x, y,\n",
    "              batch_size=128,\n",
    "              epochs=1)\n",
    "\n",
    "    # Select a text seed at random\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    generated_text = text[start_index: start_index + maxlen]\n",
    "    print('--- Generating with seed: \"' + generated_text + '\"')\n",
    "\n",
    "    for temperature in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('------ temperature:', temperature)\n",
    "        sys.stdout.write(generated_text)\n",
    "\n",
    "        # We generate 400 characters\n",
    "        for i in range(400):\n",
    "            sampled = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(generated_text):\n",
    "                sampled[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(sampled, verbose=0)[0]\n",
    "            next_index = sample(preds, temperature)\n",
    "            next_char = chars[next_index]\n",
    "\n",
    "            generated_text += next_char\n",
    "            generated_text = generated_text[1:]\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text\n",
    "\n",
    "Once the model is trained, we can use it to generate completely new text in the style of your training data.  **Train a model using your original choice of corpus below, and generate some sample sentences.** Don't worry too much about your loss / accuracy during training, but instead check on the text your model is generating. Your generated text should be somewhat coherent, i.e. similar to your training text in structure, and not excessively mispelled.\n",
    "\n",
    "An **example model architecture** is as follows (feel free to play around with different structures):\n",
    "* Embedding (for each character in your vocab) of dimension 64\n",
    "* Dropout of 20% for the embedding input to the RNN\n",
    "* 2 LSTM layers, each of dimension 512 (play around with the number and dimension of hidden layers)\n",
    "* Dropout of 50% for each LSTM layer\n",
    "* Dense softmax layer of same dimension as your vocab size (e.g. if your vocab size is 100, this layer is the probabilty that your output is one of 100 possible characters)\n",
    "    \n",
    "**You should understand what each of the above elements are and how they work at a high level by the end of this week's exercise.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalizing the exercise\n",
    "How do you think you can apply what you learned in the above exercise to other problems involving text? For example, how would you tackle the previous IMDB sentiment classification task using an RNN architecture? **Discuss below.**\n",
    "\n",
    "(*Bonus*: create an RNN model for the IMDB classification task and discuss your results. How does the performance compare to your bag of words model?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: RNNs from scratch\n",
    "Now that you understand how to use RNNs, it's time to build a basic one from scratch.  You won't understand how they work until you get stuck in the weeds! \n",
    "\n",
    "### Generate text (Version 2)\n",
    "Your task is now to **build the forward pass of a simple RNN, without using any existing RNN APIs**. You can use PyTorch or Tensorflow (Keras is too high level for this exercise), both of which will automatically handle backpropagation for you.  If you use Tensorflow, please research and use Eager execution - it replaces Tensorflow's default graph / session framework, which is very difficult to learn and debug.\n",
    "\n",
    "Similar to last week's exercise, create a class for your network (write forward and loss steps, allowing PyTorch or Tensorflow to handle backpropagation for you).  Consider appropriate sizes for your input, hidden and output layers - your __init__ method should take in the params `hidden_size`, `vocab_size`, and `embedding_size` (if you use embeddings). Using these variables, you should initialise three weight layers `input_layer`, `hidden_layer`, and `output_layer`.  In an RNN, you will also have to deal with another item - the `hidden_state`. (Note: your RNN structure may vary slightly from this depending on your learning materials, but the key part is always `hidden_state`)\n",
    "\n",
    "You should **train your RNN on the same data and task as in Chapter 3.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do the results of your basic RNN compare to your model in Chapter 3?**  What do you think explains the difference in performance? Discuss below.\n",
    "\n",
    "Some relevant resources on LSTMs (and RNN theory) below if you are interested:\n",
    "* http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "* https://www.youtube.com/watch?v=93rzMHtYT_0&list=LLpNVCNE9cYqVrjb2O8bZUGg&index=2&t=0s\n",
    "* https://www.youtube.com/watch?v=zQxm3Upr3_I\n",
    "* http://harinisuresh.com/2016/10/09/lstms/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Challenges (not required!):\n",
    "1. Build the forward pass of an LSTM, without using any existing RNN APIs (as above, with PyTorch or Tensorflow)\n",
    "1. Build a basic RNN or LSTM in Numpy - including forward pass as well as backpropogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
