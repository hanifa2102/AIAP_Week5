{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 - Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this week's exercise, we will apply neural networks to a new type of data - text. You will also pick your own corpus of text to play with.  **But first, some definitions** (there's a lot of new jargon in 'NLP'):\n",
    "* *Corpus*: The set of all text documents you want to work on\n",
    "* *Document*: An individual unit of text in your corpus\n",
    "\n",
    "Below are some **examples of corpora** to get you thinking:\n",
    "\n",
    "* Corpus of 100,000 IMDB reviews, where each document is an individual review\n",
    "* Corpus of 20 English novels, where each document is an individual novel\n",
    "* Corpus of one Malay novel, where each document is a chapter\n",
    "\n",
    "You can **browse for a corpus in the below links** (if you don't already have one in mind):\n",
    "\n",
    "* https://github.com/niderhoff/nlp-datasets\n",
    "* https://www.gutenberg.org/catalog/\n",
    "\n",
    "If this is your first time working with text, it's probably easier to deal with a corpus of many short documents - for example the IMDB review dataset, which is linked below in Chapter 2.  Play around with several corpora over the course of week, and work with something that interests you. Remember text doesn't have to be English (try other languages), or even a natural language (try code or musical notation)!\n",
    "\n",
    "**Key learning resources** for the week:\n",
    "* https://web.stanford.edu/~jurafsky/slp3/ - legendary textbook introducing key theory and concepts of working with text, up to deep learning methods\n",
    "* http://web.stanford.edu/class/cs224n/ - great course that introduces theory and concepts of text processing in the context of deep learning (can read class notes / assignments and skip videos if you are short on time) \n",
    "* https://course.fast.ai/index.html - fast.ai's introduction to deep learning (you'll have to pick out the bits about text and RNNs) is an efficient and effective way of tackling the topic\n",
    "* https://www.datacamp.com/courses/natural-language-processing-fundamentals-in-python - very hands on datacamp course that will let you practice using existing tools for NLP tasks\n",
    "\n",
    "Some **additional tools below** that can help in NLP (if you haven't found them already):\n",
    "* scikit-learn has a handy set of features for NLP\n",
    "* https://spacy.io/ - commercially oriented python package for NLP\n",
    "* https://www.nltk.org/ - slightly more academic oriented python package for NLP \n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # a conventional alias\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "from src.preprocess import getFileNames\n",
    "from scipy.cluster import  hierarchy\n",
    "from scipy.cluster.hierarchy import linkage,dendrogram,cophenet\n",
    "from scipy.spatial.distance import pdist\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: How do we turn text into data we can use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert your corpus into bags of words\n",
    "\n",
    "We can't apply any of the techniques we have learned over the past few weeks directly on raw text.  Therefore, our first task is to convert our corpus into numbers.  The simplest way to do this is to use a **bag of words**. You can see some examples of this here: https://liferay.de.dariah.eu/tatom/index.html.\n",
    "\n",
    "Once you understand the concept, convert your corpus and documents into bags of words below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using the IMDB movies reviews\n",
    "- Unsup dataset is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath='/home/hanifa/workspace/AIAP/AIAP_Week5/aclImdb/train/tt'\n",
    "# mypath='/home/jupyter/AIAP_Week5/aclImdb/train/unsup/'\n",
    "filenames=getFileNames(mypath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkVectorizer(dtm):\n",
    "    a=dtm.sum(axis=0)\n",
    "    a.sort(axis=0)\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Was having memory errors loading all the unsup data, thus just took a sample of 7K.\n",
    "- Stopwords were iteratively added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text \n",
    "my_additional_stop_words=['br','film','movie']\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(my_additional_stop_words)\n",
    "\n",
    "vectorizer = CountVectorizer(input='filename',stop_words=stop_words,max_df=1000)\n",
    "# vectorizer = CountVectorizer(input='filename',stop_words='english')'\n",
    "dtm = vectorizer.fit_transform(filenames)  # a sparse matrix \n",
    "vocab = vectorizer.get_feature_names()\n",
    "\n",
    "# type(dtm)                                         \n",
    "dtm = dtm.toarray()  # convert to a regular array \n",
    "vocab = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 1000)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checkVectorizer(dtm)\n",
    "dtm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show us your bags\n",
    "\n",
    "Show and explain what one of your documents looks like as a bag of words below.  What are the advantages and disadvantages of encoding text as bags of words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The column represents the words and the rows the document itself. \n",
    "- By summing up the column, we can total frequency across all documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bag of Words Representation__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>04</th>\n",
       "      <th>12</th>\n",
       "      <th>15</th>\n",
       "      <th>1927</th>\n",
       "      <th>1928</th>\n",
       "      <th>1931</th>\n",
       "      <th>1933</th>\n",
       "      <th>1st</th>\n",
       "      <th>20</th>\n",
       "      <th>2007</th>\n",
       "      <th>23</th>\n",
       "      <th>27</th>\n",
       "      <th>29</th>\n",
       "      <th>31</th>\n",
       "      <th>40</th>\n",
       "      <th>49</th>\n",
       "      <th>64</th>\n",
       "      <th>68</th>\n",
       "      <th>abbott</th>\n",
       "      <th>aboard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   04  12  15  1927  1928  1931  1933  1st  20  2007  23  27  29  31  40  49  \\\n",
       "0   0   0   0     2     0     0     0    0   0     0   0   0   0   1   1   1   \n",
       "1   0   0   0     0     0     0     0    0   0     0   0   0   0   0   0   0   \n",
       "2   0   0   0     0     0     0     0    0   0     0   0   0   0   0   0   0   \n",
       "3   0   0   0     0     0     1     1    0   0     0   0   0   0   0   0   0   \n",
       "4   0   0   0     0     0     0     0    0   0     0   0   0   0   0   0   0   \n",
       "\n",
       "   64  68  abbott  aboard  \n",
       "0   1   1       0       0  \n",
       "1   0   0       0       1  \n",
       "2   0   0       0       0  \n",
       "3   0   0       0       0  \n",
       "4   0   0       0       0  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame(dtm, columns=vocab)\n",
    "df[vocab[0:20]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clears Memory\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFrequentWord(dtm):\n",
    "    dtm_sum=dtm.sum(axis=0)\n",
    "    idx=dtm_sum.argmax(axis=0)\n",
    "    print(\"Most frequent word: %s\" %(vocab[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent word: boyd\n"
     ]
    }
   ],
   "source": [
    "getFrequentWord(dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cons:\n",
    " - Most frequent words are considered the most important\n",
    " - The context of the word is ignored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tell us a story with your bags\n",
    "Now that your text is in a more digestible format, you can apply previously learned techniques to better understand the corpus. **Create a brief story around your corpus, for example by using clustering techniques.** Some examples of what you can do below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use Hierarchical Clustering to understand similarity of documents in your corpus. What distance measure works best? Are the results what you expect?\n",
    "- Comparing Euclidean and Cosine, Cosine seems better as it <br/>\n",
    "[Hierarchial Clustering Code](https://datascience.stackexchange.com/questions/22828/clustering-with-cosine-similarity/22834)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Hierarchial Clustering drawbacks__\n",
    "- Hierarchial Clustering was not feasible on high dimensional datasets.\n",
    "- Below image was done on a small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doHierarchialClustering(dtm,metric,threshold):\n",
    "    Z = hierarchy.linkage(dtm,\"average\", metric=metric)\n",
    "    fig=plt.figure(figsize = (7,4))\n",
    "    ax=fig.add_subplot(111)\n",
    "    ax.set(title=\"method_str\",xlabel='Clusters',ylabel='Height')\n",
    "    dendrogram(Z,\n",
    "    #            labels=df1.index.values,\n",
    "           ax=ax,\n",
    "           truncate_mode='lastp',\n",
    "           orientation='top',\n",
    "           show_leaf_counts=True\n",
    "    )\n",
    "\n",
    "    plt.show()\n",
    "    C = hierarchy.fcluster(Z,threshold, criterion=\"distance\")\n",
    "    print(\"Number of clusters %d\" %(len(np.unique(hierarchy.fcluster(Z, threshold, criterion=\"distance\")))))\n",
    "    c, coph_dists = cophenet(Z, pdist(dtm))\n",
    "#     print (\"My name is %s and weight is %d kg!\" % ('Zara', 21))\n",
    "    print(\"Cophenet score is %f\"%(c))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters 2\n",
      "Cophenet score is 0.956333\n"
     ]
    }
   ],
   "source": [
    "doHierarchialClustering(dtm,'euclidean',threshold=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters 7\n",
      "Cophenet score is -0.235672\n"
     ]
    }
   ],
   "source": [
    "doHierarchialClustering(dtm,'cosine',threshold=.89)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Learn about *Latent Dirichlet Allocation* to extract topics from your corpora, and measure each document on how much of each topic it contains. How do you interpret these topics?\n",
    "\n",
    "Some **potential inspiration** below (but please keep your own story simple!):\n",
    "* https://liferay.de.dariah.eu/tatom/topic_model_mallet.html covers a few examples of text analysis\n",
    "* http://fantheory.viacom.com/\n",
    "* https://pudding.cool/2017/02/vocabulary/\n",
    "\n",
    "Additional resources on LDA (if you are interested): \n",
    "* https://medium.com/@lettier/how-does-lda-work-ill-explain-using-emoji-108abf40fa7d\n",
    "* https://www.youtube.com/watch?v=DDq3OVp9dNA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[LDA Code](https://medium.com/mlreview/topic-modeling-with-scikit-learn-e80d33668730)\n",
    "\n",
    "[Grid Search LDA](https://www.machinelearningplus.com/nlp/topic-modeling-python-sklearn-examples/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print (\"Topic %d:\" % (topic_idx))\n",
    "        print (\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
      "             evaluate_every=-1, learning_decay=0.7,\n",
      "             learning_method='online', learning_offset=10.0,\n",
      "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
      "             n_components=10, n_jobs=-1, n_topics=2, perp_tol=0.1,\n",
      "             random_state=100, topic_word_prior=None,\n",
      "             total_samples=1000000.0, verbose=0)\n",
      "Log Likelihood:  -11641.84002821157\n",
      "Perplexity:  1130.139816415517\n",
      "{'batch_size': 128, 'doc_topic_prior': None, 'evaluate_every': -1, 'learning_decay': 0.7, 'learning_method': 'online', 'learning_offset': 10.0, 'max_doc_update_iter': 100, 'max_iter': 10, 'mean_change_tol': 0.001, 'n_components': 10, 'n_jobs': -1, 'n_topics': 2, 'perp_tol': 0.1, 'random_state': 100, 'topic_word_prior': None, 'total_samples': 1000000.0, 'verbose': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hanifa/anaconda3/lib/python3.7/site-packages/sklearn/decomposition/online_lda.py:314: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "n_components=2\n",
    "# lda_model= LatentDirichletAllocation(n_topics=n_components, max_iter=5, learning_method='online',\n",
    "#                                 learning_offset=50.,random_state=0,n_jobs=-1).fit(dtm)\n",
    "lda_model = LatentDirichletAllocation(n_topics=2,               # Number of topics\n",
    "                                      max_iter=10,               # Max learning iterations\n",
    "                                      learning_method='online',   \n",
    "                                      random_state=100,          # Random state\n",
    "                                      batch_size=128,            # n docs in each learning iter\n",
    "                                      evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n",
    "                                      n_jobs = -1,               # Use all available CPUs\n",
    "                                     )\n",
    "lda_output = lda_model.fit_transform(dtm)\n",
    "\n",
    "print(lda_model)  # Model attributes\n",
    "\n",
    "\n",
    "# # Log Likelyhood: Higher the better\n",
    "print(\"Log Likelihood: \", lda_model.score(dtm))\n",
    "\n",
    "# Perplexity: Lower the better. Perplexity = exp(-1. * log-likelihood per word)\n",
    "print(\"Perplexity: \", lda_model.perplexity(dtm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='online', learning_offset=10.0,\n",
       "             max_doc_update_iter=100, max_iter=10, mean_change_tol=0.001,\n",
       "             n_components=10, n_jobs=-1, n_topics=2, perp_tol=0.1,\n",
       "             random_state=100, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_top_words = 100\n",
    "display_topics(lda, vocab, no_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__GridSearch the LDA Model to find best fit__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GridSearchLDA(dtm):\n",
    "    # Define Search Param\n",
    "    search_params = {'n_components': [10, 15, 20, 25, 30], 'learning_decay': [.5, .7, .9]}\n",
    "\n",
    "    # Init the Model\n",
    "    lda = LatentDirichletAllocation()\n",
    "\n",
    "    # Init Grid Search Class\n",
    "    model = GridSearchCV(lda, param_grid=search_params)\n",
    "\n",
    "    # Do the Grid Search\n",
    "    model.fit(dtm)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hanifa/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model's Params:  {'learning_decay': 0.7, 'n_components': 10}\n",
      "Best Log Likelihood Score:  -7928.47122813816\n",
      "Model Perplexity:  1486.4210098579993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hanifa/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "model = GridSearchLDA(dtm)\n",
    "# Best Model\n",
    "best_lda_model = model.best_estimator_\n",
    "\n",
    "# Model Parameters\n",
    "print(\"Best Model's Params: \", model.best_params_)\n",
    "\n",
    "# Log Likelihood Score\n",
    "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
    "\n",
    "# Perplexity\n",
    "print(\"Model Perplexity: \", best_lda_model.perplexity(dtm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top n keywords for each topic\n",
    "def show_topics(vectorizer=vectorizer, lda_model=lda_model, n_words=20):\n",
    "    keywords = np.array(vectorizer.get_feature_names())\n",
    "    topic_keywords = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_keyword_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_keywords.append(keywords.take(top_keyword_locs))\n",
    "    return topic_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 0</th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Word 3</th>\n",
       "      <th>Word 4</th>\n",
       "      <th>Word 5</th>\n",
       "      <th>Word 6</th>\n",
       "      <th>Word 7</th>\n",
       "      <th>Word 8</th>\n",
       "      <th>Word 9</th>\n",
       "      <th>Word 10</th>\n",
       "      <th>Word 11</th>\n",
       "      <th>Word 12</th>\n",
       "      <th>Word 13</th>\n",
       "      <th>Word 14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic 0</th>\n",
       "      <td>silent</td>\n",
       "      <td>boyd</td>\n",
       "      <td>war</td>\n",
       "      <td>world</td>\n",
       "      <td>comedy</td>\n",
       "      <td>era</td>\n",
       "      <td>timeless</td>\n",
       "      <td>like</td>\n",
       "      <td>time</td>\n",
       "      <td>characters</td>\n",
       "      <td>cassidy</td>\n",
       "      <td>hopalong</td>\n",
       "      <td>william</td>\n",
       "      <td>films</td>\n",
       "      <td>role</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 1</th>\n",
       "      <td>boyd</td>\n",
       "      <td>william</td>\n",
       "      <td>soldiers</td>\n",
       "      <td>astor</td>\n",
       "      <td>silent</td>\n",
       "      <td>look</td>\n",
       "      <td>like</td>\n",
       "      <td>picture</td>\n",
       "      <td>great</td>\n",
       "      <td>boris</td>\n",
       "      <td>purser</td>\n",
       "      <td>princess</td>\n",
       "      <td>wolheim</td>\n",
       "      <td>karloff</td>\n",
       "      <td>lot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 2</th>\n",
       "      <td>movies</td>\n",
       "      <td>heroes</td>\n",
       "      <td>like</td>\n",
       "      <td>early</td>\n",
       "      <td>story</td>\n",
       "      <td>scene</td>\n",
       "      <td>just</td>\n",
       "      <td>did</td>\n",
       "      <td>great</td>\n",
       "      <td>decades</td>\n",
       "      <td>classic</td>\n",
       "      <td>silent</td>\n",
       "      <td>modern</td>\n",
       "      <td>muslim</td>\n",
       "      <td>key</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 3</th>\n",
       "      <td>character</td>\n",
       "      <td>toni</td>\n",
       "      <td>work</td>\n",
       "      <td>end</td>\n",
       "      <td>use</td>\n",
       "      <td>familiar</td>\n",
       "      <td>words</td>\n",
       "      <td>lisa</td>\n",
       "      <td>kudrow</td>\n",
       "      <td>age</td>\n",
       "      <td>office</td>\n",
       "      <td>comic</td>\n",
       "      <td>advantage</td>\n",
       "      <td>current</td>\n",
       "      <td>society</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 4</th>\n",
       "      <td>surprised</td>\n",
       "      <td>like</td>\n",
       "      <td>office</td>\n",
       "      <td>actually</td>\n",
       "      <td>good</td>\n",
       "      <td>scenes</td>\n",
       "      <td>funny</td>\n",
       "      <td>just</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>unexpected</td>\n",
       "      <td>clockwatchers</td>\n",
       "      <td>thing</td>\n",
       "      <td>feel</td>\n",
       "      <td>posey</td>\n",
       "      <td>watch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 5</th>\n",
       "      <td>comedy</td>\n",
       "      <td>ship</td>\n",
       "      <td>louis</td>\n",
       "      <td>direction</td>\n",
       "      <td>wolheim</td>\n",
       "      <td>escape</td>\n",
       "      <td>young</td>\n",
       "      <td>beautiful</td>\n",
       "      <td>won</td>\n",
       "      <td>adventure</td>\n",
       "      <td>stuck</td>\n",
       "      <td>best</td>\n",
       "      <td>look</td>\n",
       "      <td>end</td>\n",
       "      <td>prison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 6</th>\n",
       "      <td>thought</td>\n",
       "      <td>office</td>\n",
       "      <td>german</td>\n",
       "      <td>movies</td>\n",
       "      <td>comedy</td>\n",
       "      <td>space</td>\n",
       "      <td>best</td>\n",
       "      <td>just</td>\n",
       "      <td>academy</td>\n",
       "      <td>award</td>\n",
       "      <td>funny</td>\n",
       "      <td>camp</td>\n",
       "      <td>recently</td>\n",
       "      <td>arabia</td>\n",
       "      <td>fighting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 7</th>\n",
       "      <td>life</td>\n",
       "      <td>dreams</td>\n",
       "      <td>actors</td>\n",
       "      <td>wonderful</td>\n",
       "      <td>roles</td>\n",
       "      <td>make</td>\n",
       "      <td>didn</td>\n",
       "      <td>working</td>\n",
       "      <td>close</td>\n",
       "      <td>character</td>\n",
       "      <td>performances</td>\n",
       "      <td>trying</td>\n",
       "      <td>big</td>\n",
       "      <td>busy</td>\n",
       "      <td>actresses</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 8</th>\n",
       "      <td>like</td>\n",
       "      <td>humor</td>\n",
       "      <td>life</td>\n",
       "      <td>shown</td>\n",
       "      <td>realism</td>\n",
       "      <td>just</td>\n",
       "      <td>best</td>\n",
       "      <td>thought</td>\n",
       "      <td>office</td>\n",
       "      <td>look</td>\n",
       "      <td>drama</td>\n",
       "      <td>character</td>\n",
       "      <td>beautiful</td>\n",
       "      <td>watch</td>\n",
       "      <td>space</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 9</th>\n",
       "      <td>wolheim</td>\n",
       "      <td>milestone</td>\n",
       "      <td>louis</td>\n",
       "      <td>astor</td>\n",
       "      <td>escape</td>\n",
       "      <td>boyd</td>\n",
       "      <td>scene</td>\n",
       "      <td>men</td>\n",
       "      <td>best</td>\n",
       "      <td>william</td>\n",
       "      <td>mary</td>\n",
       "      <td>scenes</td>\n",
       "      <td>arabian</td>\n",
       "      <td>knights</td>\n",
       "      <td>way</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Word 0     Word 1    Word 2     Word 3   Word 4    Word 5  \\\n",
       "Topic 0     silent       boyd       war      world   comedy       era   \n",
       "Topic 1       boyd    william  soldiers      astor   silent      look   \n",
       "Topic 2     movies     heroes      like      early    story     scene   \n",
       "Topic 3  character       toni      work        end      use  familiar   \n",
       "Topic 4  surprised       like    office   actually     good    scenes   \n",
       "Topic 5     comedy       ship     louis  direction  wolheim    escape   \n",
       "Topic 6    thought     office    german     movies   comedy     space   \n",
       "Topic 7       life     dreams    actors  wonderful    roles      make   \n",
       "Topic 8       like      humor      life      shown  realism      just   \n",
       "Topic 9    wolheim  milestone     louis      astor   escape      boyd   \n",
       "\n",
       "           Word 6     Word 7    Word 8      Word 9        Word 10    Word 11  \\\n",
       "Topic 0  timeless       like      time  characters        cassidy   hopalong   \n",
       "Topic 1      like    picture     great       boris         purser   princess   \n",
       "Topic 2      just        did     great     decades        classic     silent   \n",
       "Topic 3     words       lisa    kudrow         age         office      comic   \n",
       "Topic 4     funny       just  accuracy  unexpected  clockwatchers      thing   \n",
       "Topic 5     young  beautiful       won   adventure          stuck       best   \n",
       "Topic 6      best       just   academy       award          funny       camp   \n",
       "Topic 7      didn    working     close   character   performances     trying   \n",
       "Topic 8      best    thought    office        look          drama  character   \n",
       "Topic 9     scene        men      best     william           mary     scenes   \n",
       "\n",
       "           Word 12  Word 13    Word 14  \n",
       "Topic 0    william    films       role  \n",
       "Topic 1    wolheim  karloff        lot  \n",
       "Topic 2     modern   muslim        key  \n",
       "Topic 3  advantage  current    society  \n",
       "Topic 4       feel    posey      watch  \n",
       "Topic 5       look      end     prison  \n",
       "Topic 6   recently   arabia   fighting  \n",
       "Topic 7        big     busy  actresses  \n",
       "Topic 8  beautiful    watch      space  \n",
       "Topic 9    arabian  knights        way  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_keywords = show_topics(vectorizer=vectorizer, lda_model=best_lda_model, n_words=15)        \n",
    "\n",
    "# Topic - Keywords Dataframe\n",
    "df_topic_keywords = pd.DataFrame(topic_keywords)\n",
    "df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\n",
    "df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\n",
    "df_topic_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Should remove common words like br and film and movie\n",
    "\n",
    "- Also use some gridSearch to find optimal N-components (use perplexity score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize your bags\n",
    "In the above exercise, you may find it important to normalize your data.  One useful method when dealing with text is *Term Frequency - Inverse Document Frequency (TF-IDF)*. You can see more detail on this here: http://blog.christianperone.com/2011/10/machine-learning-text-feature-extraction-tf-idf-part-ii/.\n",
    "\n",
    "Once you understand the concept, **express your data as TF-IDF vectors (instead of simple bag-of-words counts), and see if it changes your above story**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_tfid = TfidfTransformer().fit_transform(dtm)\n",
    "dtm_tfid = dtm_tfid.toarray() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>04</th>\n",
       "      <th>12</th>\n",
       "      <th>15</th>\n",
       "      <th>1927</th>\n",
       "      <th>1928</th>\n",
       "      <th>1931</th>\n",
       "      <th>1933</th>\n",
       "      <th>1st</th>\n",
       "      <th>20</th>\n",
       "      <th>2007</th>\n",
       "      <th>23</th>\n",
       "      <th>27</th>\n",
       "      <th>29</th>\n",
       "      <th>31</th>\n",
       "      <th>40</th>\n",
       "      <th>49</th>\n",
       "      <th>64</th>\n",
       "      <th>68</th>\n",
       "      <th>abbott</th>\n",
       "      <th>aboard</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.133664</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.093679</td>\n",
       "      <td>0.093679</td>\n",
       "      <td>0.093679</td>\n",
       "      <td>0.093679</td>\n",
       "      <td>0.093679</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.082943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135802</td>\n",
       "      <td>0.135802</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    04   12   15      1927  1928      1931      1933  1st   20  2007   23  \\\n",
       "0  0.0  0.0  0.0  0.133664   0.0  0.000000  0.000000  0.0  0.0   0.0  0.0   \n",
       "1  0.0  0.0  0.0  0.000000   0.0  0.000000  0.000000  0.0  0.0   0.0  0.0   \n",
       "2  0.0  0.0  0.0  0.000000   0.0  0.000000  0.000000  0.0  0.0   0.0  0.0   \n",
       "3  0.0  0.0  0.0  0.000000   0.0  0.135802  0.135802  0.0  0.0   0.0  0.0   \n",
       "4  0.0  0.0  0.0  0.000000   0.0  0.000000  0.000000  0.0  0.0   0.0  0.0   \n",
       "\n",
       "    27   29        31        40        49        64        68  abbott  \\\n",
       "0  0.0  0.0  0.093679  0.093679  0.093679  0.093679  0.093679     0.0   \n",
       "1  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000     0.0   \n",
       "2  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000     0.0   \n",
       "3  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000     0.0   \n",
       "4  0.0  0.0  0.000000  0.000000  0.000000  0.000000  0.000000     0.0   \n",
       "\n",
       "     aboard  \n",
       "0  0.000000  \n",
       "1  0.082943  \n",
       "2  0.000000  \n",
       "3  0.000000  \n",
       "4  0.000000  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame(dtm_tfid, columns=vocab)\n",
    "df[vocab[0:20]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent word: comedy\n"
     ]
    }
   ],
   "source": [
    "getFrequentWord(dtm_tfid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hanifa/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model's Params:  {'learning_decay': 0.5, 'n_components': 10}\n",
      "Best Log Likelihood Score:  -887.2045828397883\n",
      "Model Perplexity:  194580.64651439677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hanifa/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "model = GridSearchLDA(dtm_tfid)\n",
    "# Best Model\n",
    "best_lda_model = model.best_estimator_\n",
    "\n",
    "# Model Parameters\n",
    "print(\"Best Model's Params: \", model.best_params_)\n",
    "\n",
    "# Log Likelihood Score\n",
    "print(\"Best Log Likelihood Score: \", model.best_score_)\n",
    "\n",
    "# Perplexity\n",
    "print(\"Model Perplexity: \", best_lda_model.perplexity(dtm_tfid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 0</th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Word 3</th>\n",
       "      <th>Word 4</th>\n",
       "      <th>Word 5</th>\n",
       "      <th>Word 6</th>\n",
       "      <th>Word 7</th>\n",
       "      <th>Word 8</th>\n",
       "      <th>Word 9</th>\n",
       "      <th>Word 10</th>\n",
       "      <th>Word 11</th>\n",
       "      <th>Word 12</th>\n",
       "      <th>Word 13</th>\n",
       "      <th>Word 14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic 0</th>\n",
       "      <td>just</td>\n",
       "      <td>great</td>\n",
       "      <td>look</td>\n",
       "      <td>silent</td>\n",
       "      <td>time</td>\n",
       "      <td>adventure</td>\n",
       "      <td>louis</td>\n",
       "      <td>direction</td>\n",
       "      <td>early</td>\n",
       "      <td>films</td>\n",
       "      <td>like</td>\n",
       "      <td>camp</td>\n",
       "      <td>shown</td>\n",
       "      <td>thought</td>\n",
       "      <td>character</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 1</th>\n",
       "      <td>princess</td>\n",
       "      <td>boyd</td>\n",
       "      <td>high</td>\n",
       "      <td>wolheim</td>\n",
       "      <td>knights</td>\n",
       "      <td>make</td>\n",
       "      <td>arabian</td>\n",
       "      <td>astor</td>\n",
       "      <td>look</td>\n",
       "      <td>william</td>\n",
       "      <td>comedy</td>\n",
       "      <td>upper</td>\n",
       "      <td>rediscovered</td>\n",
       "      <td>art</td>\n",
       "      <td>private</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 2</th>\n",
       "      <td>dreams</td>\n",
       "      <td>manage</td>\n",
       "      <td>life</td>\n",
       "      <td>way</td>\n",
       "      <td>worked</td>\n",
       "      <td>managed</td>\n",
       "      <td>quite</td>\n",
       "      <td>make</td>\n",
       "      <td>speedy</td>\n",
       "      <td>story</td>\n",
       "      <td>escape</td>\n",
       "      <td>female</td>\n",
       "      <td>jobs</td>\n",
       "      <td>gets</td>\n",
       "      <td>ultimately</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 3</th>\n",
       "      <td>silent</td>\n",
       "      <td>lot</td>\n",
       "      <td>boyd</td>\n",
       "      <td>fighting</td>\n",
       "      <td>war</td>\n",
       "      <td>comedy</td>\n",
       "      <td>wollheim</td>\n",
       "      <td>inventive</td>\n",
       "      <td>world</td>\n",
       "      <td>lewis</td>\n",
       "      <td>era</td>\n",
       "      <td>timeless</td>\n",
       "      <td>1st</td>\n",
       "      <td>milestone</td>\n",
       "      <td>best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 4</th>\n",
       "      <td>good</td>\n",
       "      <td>posey</td>\n",
       "      <td>heroes</td>\n",
       "      <td>funny</td>\n",
       "      <td>work</td>\n",
       "      <td>surprised</td>\n",
       "      <td>scenes</td>\n",
       "      <td>solid</td>\n",
       "      <td>familiar</td>\n",
       "      <td>movies</td>\n",
       "      <td>story</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>character</td>\n",
       "      <td>office</td>\n",
       "      <td>german</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 5</th>\n",
       "      <td>dozen</td>\n",
       "      <td>early</td>\n",
       "      <td>admit</td>\n",
       "      <td>lead</td>\n",
       "      <td>variation</td>\n",
       "      <td>schtick</td>\n",
       "      <td>background</td>\n",
       "      <td>silents</td>\n",
       "      <td>lights</td>\n",
       "      <td>supposed</td>\n",
       "      <td>dressed</td>\n",
       "      <td>majority</td>\n",
       "      <td>major</td>\n",
       "      <td>viewed</td>\n",
       "      <td>beefcake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 6</th>\n",
       "      <td>men</td>\n",
       "      <td>wolheim</td>\n",
       "      <td>knights</td>\n",
       "      <td>pows</td>\n",
       "      <td>caucasian</td>\n",
       "      <td>milestone</td>\n",
       "      <td>arabian</td>\n",
       "      <td>astor</td>\n",
       "      <td>scene</td>\n",
       "      <td>lewis</td>\n",
       "      <td>ship</td>\n",
       "      <td>mary</td>\n",
       "      <td>scenes</td>\n",
       "      <td>william</td>\n",
       "      <td>louis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 7</th>\n",
       "      <td>just</td>\n",
       "      <td>great</td>\n",
       "      <td>look</td>\n",
       "      <td>silent</td>\n",
       "      <td>time</td>\n",
       "      <td>adventure</td>\n",
       "      <td>louis</td>\n",
       "      <td>direction</td>\n",
       "      <td>early</td>\n",
       "      <td>films</td>\n",
       "      <td>like</td>\n",
       "      <td>camp</td>\n",
       "      <td>shown</td>\n",
       "      <td>thought</td>\n",
       "      <td>character</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 8</th>\n",
       "      <td>ship</td>\n",
       "      <td>doesn</td>\n",
       "      <td>like</td>\n",
       "      <td>comedy</td>\n",
       "      <td>office</td>\n",
       "      <td>beautiful</td>\n",
       "      <td>purser</td>\n",
       "      <td>1927</td>\n",
       "      <td>life</td>\n",
       "      <td>boris</td>\n",
       "      <td>karloff</td>\n",
       "      <td>dangerfield</td>\n",
       "      <td>pvt</td>\n",
       "      <td>hush</td>\n",
       "      <td>gas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 9</th>\n",
       "      <td>movies</td>\n",
       "      <td>office</td>\n",
       "      <td>rent</td>\n",
       "      <td>ve</td>\n",
       "      <td>space</td>\n",
       "      <td>mind</td>\n",
       "      <td>watching</td>\n",
       "      <td>watch</td>\n",
       "      <td>thought</td>\n",
       "      <td>apparently</td>\n",
       "      <td>whispers</td>\n",
       "      <td>superb</td>\n",
       "      <td>word</td>\n",
       "      <td>villain</td>\n",
       "      <td>product</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Word 0   Word 1   Word 2    Word 3     Word 4     Word 5  \\\n",
       "Topic 0      just    great     look    silent       time  adventure   \n",
       "Topic 1  princess     boyd     high   wolheim    knights       make   \n",
       "Topic 2    dreams   manage     life       way     worked    managed   \n",
       "Topic 3    silent      lot     boyd  fighting        war     comedy   \n",
       "Topic 4      good    posey   heroes     funny       work  surprised   \n",
       "Topic 5     dozen    early    admit      lead  variation    schtick   \n",
       "Topic 6       men  wolheim  knights      pows  caucasian  milestone   \n",
       "Topic 7      just    great     look    silent       time  adventure   \n",
       "Topic 8      ship    doesn     like    comedy     office  beautiful   \n",
       "Topic 9    movies   office     rent        ve      space       mind   \n",
       "\n",
       "             Word 6     Word 7    Word 8      Word 9   Word 10      Word 11  \\\n",
       "Topic 0       louis  direction     early       films      like         camp   \n",
       "Topic 1     arabian      astor      look     william    comedy        upper   \n",
       "Topic 2       quite       make    speedy       story    escape       female   \n",
       "Topic 3    wollheim  inventive     world       lewis       era     timeless   \n",
       "Topic 4      scenes      solid  familiar      movies     story     accuracy   \n",
       "Topic 5  background    silents    lights    supposed   dressed     majority   \n",
       "Topic 6     arabian      astor     scene       lewis      ship         mary   \n",
       "Topic 7       louis  direction     early       films      like         camp   \n",
       "Topic 8      purser       1927      life       boris   karloff  dangerfield   \n",
       "Topic 9    watching      watch   thought  apparently  whispers       superb   \n",
       "\n",
       "              Word 12    Word 13     Word 14  \n",
       "Topic 0         shown    thought   character  \n",
       "Topic 1  rediscovered        art     private  \n",
       "Topic 2          jobs       gets  ultimately  \n",
       "Topic 3           1st  milestone        best  \n",
       "Topic 4     character     office      german  \n",
       "Topic 5         major     viewed    beefcake  \n",
       "Topic 6        scenes    william       louis  \n",
       "Topic 7         shown    thought   character  \n",
       "Topic 8           pvt       hush         gas  \n",
       "Topic 9          word    villain     product  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_keywords = show_topics(vectorizer=vectorizer, lda_model=best_lda_model, n_words=15)        \n",
    "\n",
    "# Topic - Keywords Dataframe\n",
    "df_topic_keywords = pd.DataFrame(topic_keywords)\n",
    "df_topic_keywords.columns = ['Word '+str(i) for i in range(df_topic_keywords.shape[1])]\n",
    "df_topic_keywords.index = ['Topic '+str(i) for i in range(df_topic_keywords.shape[0])]\n",
    "df_topic_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show us your bags (Version 2)\n",
    "\n",
    "Show and explain what one of your documents looks like as a TF-IDF vector below.  How is this different from a simple bag-of-words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtm[vocab=='br']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(dtm_tfid[2,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(dtm[2,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_tfid_sum = dtm_tfid.sum(axis=0)\n",
    "vocab[dtm_tfid_sum.argmax(axis=0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Simple Supervised Learning with Text\n",
    "Now that you are comfortable with treating text as numbers, we can try out supervised learning.  We'll use a labelled dataset of IMDB reviews to classify each review as 'positive' or 'negative'.  You can **find the data below:**\n",
    "\n",
    "http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "\n",
    "Load in and process the data, then train a supervised learning model.  **You should achieve val or test set accuracy of 85%**. Pretty good for a simple bag, no?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Sentiment Analysis in IMDB)[https://itnext.io/machine-learning-sentiment-analysis-of-movie-reviews-using-logisticregression-62e9622b4532]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Playing with Recurrent Neural Networks (RNN)\n",
    "So far, we've only treated text as a simple bag, with reasonable results.  We'll now shift to a more complex representation of language: recurrent neural networks.  To do so, we need to process text at the word or character level, and capture the sequence of a document. \n",
    "\n",
    "Our task here is to build an RNN that 'eats up' sequences of characters in order to predict the next character in a sequence, for every step in the sequence of a document. This is a common (and fun) task, with lots of examples available online. \n",
    "\n",
    "For this task, use existing RNN APIs (don't code everything from scratch) from Keras or PyTorch. \n",
    "\n",
    "**Read up on RNNs and this exercise** below:\n",
    "* http://karpathy.github.io/2015/05/21/rnn-effectiveness/ - start here!\n",
    "* https://github.com/martin-gorner/tensorflow-rnn-shakespeare - video, slides and code going through an example with Shakespeare\n",
    "* http://killianlevacher.github.io/blog/posts/post-2016-03-01/post.html - another nice example based on Trump tweets\n",
    "\n",
    "### Prepare your data\n",
    "\n",
    "Our first step is to prepare our text. **Process your corpora into a format that can be used by an RNN, and walkthough one sequence below**.\n",
    "\n",
    "An **example way to shape your data** for this task is as follows (feel free to play around with different structures):\n",
    "\n",
    "*In this example your corpora starts with the string 'the cat and I'*\n",
    "* RNN input: divide your text into sequences of 10 characters e.g. 'the cat an'\n",
    "* RNN output: the 1 character immediately following RNN input sequences e.g. 'd'. \n",
    "* Note: You may or may not want to divide your text into overlapping strings (e.g. RNN input contains 'the cat an', 'he cat and', 'e cat and ', ...) . How is the model different in each case?\n",
    "* Note: Your 'vocabulary' or `vocab_size` here is the number of unique characters in your text (and therefore the number of classes you want to predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text\n",
    "\n",
    "Once the model is trained, we can use it to generate completely new text in the style of your training data.  **Train a model using your original choice of corpus below, and generate some sample sentences.** Don't worry too much about your loss / accuracy during training, but instead check on the text your model is generating. Your generated text should be somewhat coherent, i.e. similar to your training text in structure, and not excessively mispelled.\n",
    "\n",
    "An **example model architecture** is as follows (feel free to play around with different structures):\n",
    "* Embedding (for each character in your vocab) of dimension 64\n",
    "* Dropout of 20% for the embedding input to the RNN\n",
    "* 2 LSTM layers, each of dimension 512 (play around with the number and dimension of hidden layers)\n",
    "* Dropout of 50% for each LSTM layer\n",
    "* Dense softmax layer of same dimension as your vocab size (e.g. if your vocab size is 100, this layer is the probabilty that your output is one of 100 possible characters)\n",
    "    \n",
    "**You should understand what each of the above elements are and how they work at a high level by the end of this week's exercise.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalizing the exercise\n",
    "How do you think you can apply what you learned in the above exercise to other problems involving text? For example, how would you tackle the previous IMDB sentiment classification task using an RNN architecture? **Discuss below.**\n",
    "\n",
    "(*Bonus*: create an RNN model for the IMDB classification task and discuss your results. How does the performance compare to your bag of words model?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: RNNs from scratch\n",
    "Now that you understand how to use RNNs, it's time to build a basic one from scratch.  You won't understand how they work until you get stuck in the weeds! \n",
    "\n",
    "### Generate text (Version 2)\n",
    "Your task is now to **build the forward pass of a simple RNN, without using any existing RNN APIs**. You can use PyTorch or Tensorflow (Keras is too high level for this exercise), both of which will automatically handle backpropagation for you.  If you use Tensorflow, please research and use Eager execution - it replaces Tensorflow's default graph / session framework, which is very difficult to learn and debug.\n",
    "\n",
    "Similar to last week's exercise, create a class for your network (write forward and loss steps, allowing PyTorch or Tensorflow to handle backpropagation for you).  Consider appropriate sizes for your input, hidden and output layers - your __init__ method should take in the params `hidden_size`, `vocab_size`, and `embedding_size` (if you use embeddings). Using these variables, you should initialise three weight layers `input_layer`, `hidden_layer`, and `output_layer`.  In an RNN, you will also have to deal with another item - the `hidden_state`. (Note: your RNN structure may vary slightly from this depending on your learning materials, but the key part is always `hidden_state`)\n",
    "\n",
    "You should **train your RNN on the same data and task as in Chapter 3.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do the results of your basic RNN compare to your model in Chapter 3?**  What do you think explains the difference in performance? Discuss below.\n",
    "\n",
    "Some relevant resources on LSTMs (and RNN theory) below if you are interested:\n",
    "* http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "* https://www.youtube.com/watch?v=93rzMHtYT_0&list=LLpNVCNE9cYqVrjb2O8bZUGg&index=2&t=0s\n",
    "* https://www.youtube.com/watch?v=zQxm3Upr3_I\n",
    "* http://harinisuresh.com/2016/10/09/lstms/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Challenges (not required!):\n",
    "1. Build the forward pass of an LSTM, without using any existing RNN APIs (as above, with PyTorch or Tensorflow)\n",
    "1. Build a basic RNN or LSTM in Numpy - including forward pass as well as backpropogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
